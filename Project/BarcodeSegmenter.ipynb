{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from google_drive_downloader import GoogleDriveDownloader as gdd\n",
    "\n",
    "import cv2\n",
    "\n",
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.models import Sequential, clone_model\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, BatchNormalization, UpSampling2D\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import MeanSquaredError\n",
    "\n",
    "from tensorflow.keras.callbacks import TensorBoard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Barcode Image Segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abstract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Barcodes are widely used to localize items on images. Numerous applications are available to find and recognize the barcodes. However they cannot manage cases such as targets with small scale, oclusions, shape deformations, noise and blurring. The most widely solutions require the barcode to be oriented in single directions and may fail in conditions that seem without problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This work attempts to solve the problem of barcode localization using a deep learning based segmentation approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change to the folder where the images datasets are loaded/present and where the \n",
    "WORKPLACE_FOLDER = \"/tmp\"\n",
    "\n",
    "# Folder to store/read checkpoints for model training\n",
    "CHECKPOINTS_DIR = \"checkpoints\"\n",
    "\n",
    "# Image size for the input\n",
    "IMAGE_SIZE = (224, 224)\n",
    "\n",
    "# Image size + channels for the input\n",
    "INPUT_IMAGE_SIZE = (IMAGE_SIZE[0], IMAGE_SIZE[1], 3)\n",
    "\n",
    "RESUME_MODELS = True\n",
    "TRAIN_MODELS = False\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "NUM_EPOCHS = 60"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In total five datasets are used - two to train the neural network and three to test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First three datasets are downloaded from http://artelab.dista.uninsubria.it/downloads/datasets/barcode/medium_barcode_1d/medium_barcode_1d.html (ARTELAB) [[1](#ref_1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The masks for the segments containing barcodes were created using the [pyzbar](https://pypi.org/project/pyzbar/pyzbar) library. For this purpose the image was rotated with predefined steps and the obtained from [pyzbar](https://pypi.org/project/pyzbar/pyzbar) points were used to set the required segments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It has to be noted that the [pyzbar](https://pypi.org/project/pyzbar/pyzbar) did not produced reliable results even for some images that seemed without defects. For this reason not all the images from [ARTELAB](http://artelab.dista.uninsubria.it/downloads/datasets/barcode/medium_barcode_1d/medium_barcode_1d.html) were used. Only the successfully generated masks were applied."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pictures of barcodes taken from devices with autofocus - first subset.\n",
    "Contains 122 images (originals + masks) with zipped size 38,5 MB "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading 18MFEr2iekIojLwEhzswOIyW_Fp8CNQA5 into /tmp/Dataset1.zip... Done.\n",
      "Unzipping...Done.\n"
     ]
    }
   ],
   "source": [
    "gdd.download_file_from_google_drive(file_id=\"18MFEr2iekIojLwEhzswOIyW_Fp8CNQA5\",\n",
    "                                    dest_path=os.path.join(WORKPLACE_FOLDER, \"Dataset1.zip\"),\n",
    "                                    unzip=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pictures of barcodes taken from devices with autofocus - first subset.\n",
    "Contains 76 images (originals + masks) with zipped size 124,5 MB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading 1SHJi744MZV40Mp38m6RW8PeuQktbEt6u into /tmp/Dataset2.zip... Done.\n",
      "Unzipping...Done.\n"
     ]
    }
   ],
   "source": [
    "gdd.download_file_from_google_drive(file_id=\"1SHJi744MZV40Mp38m6RW8PeuQktbEt6u\",\n",
    "                                    dest_path=os.path.join(WORKPLACE_FOLDER, \"Dataset2.zip\"),\n",
    "                                    unzip=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pictures of barcodes taken from devices without autofocus. Contains 61 images (originals + masks) with zipped size 13,6 MB "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading 1ybx4TiciMoQcpVi3fAzZoUOuSg2WPrvI into /tmp/Dataset3.zip... Done.\n",
      "Unzipping...Done.\n"
     ]
    }
   ],
   "source": [
    "gdd.download_file_from_google_drive(file_id=\"1ybx4TiciMoQcpVi3fAzZoUOuSg2WPrvI\",\n",
    "                                    dest_path=os.path.join(WORKPLACE_FOLDER, \"Dataset3.zip\"),\n",
    "                                    unzip=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Downloaded from https://github.com/rohrlaf/SlaRle.js/tree/master/Muenster%20BarcodeDB and referenced as Muenster BarodeDB [[2](#ref_2)]. Masks were prepared following the above mentioned procedure with [pyzbar](https://pypi.org/project/pyzbar/pyzbar) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Contains 863 images (originals + masks) with zipped size 46,9 MB "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading 1gfxKTaG7tHDDK5fPQW6PH-Zcbx7KPXzO into /tmp/Dataset4.zip... Done.\n",
      "Unzipping...Done.\n"
     ]
    }
   ],
   "source": [
    "gdd.download_file_from_google_drive(file_id=\"1gfxKTaG7tHDDK5fPQW6PH-Zcbx7KPXzO\",\n",
    "                                    dest_path=os.path.join(WORKPLACE_FOLDER, \"Dataset4.zip\"),\n",
    "                                    unzip=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Downloaded from http://artelab.dista.uninsubria.it/downloads/datasets/barcode/hough_barcode_1d/hough_barcode_1d.html (ARTELAB) [[3](#ref_3)] Referenced as dataset no.2 plain (1d_barcode_extended_plain.zip) contains only the images and the detection masks. Masks had to be adjusted to be grayscale one channel images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Contains 365 images (originals + masks) with zipped size 37,5 MB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading 1rNi26q-iq5Q4BtrIOT-pDSleCtKzw3pk into /tmp/Dataset5.zip... Done.\n",
      "Unzipping...Done.\n"
     ]
    }
   ],
   "source": [
    "gdd.download_file_from_google_drive(file_id=\"1rNi26q-iq5Q4BtrIOT-pDSleCtKzw3pk\",\n",
    "                                    dest_path=os.path.join(WORKPLACE_FOLDER, \"Dataset5.zip\"),\n",
    "                                    unzip=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_filenames(base_dir):\n",
    "    \"\"\"\n",
    "    Returns the filenames for barcodes and masks\n",
    "    Assumes the following structure:\n",
    "    |---base_dir\n",
    "    | |---Original\n",
    "    | | |---image1.jpg\n",
    "    | | |---image2.jpg\n",
    "    | |---Detection\n",
    "    | | |---image1.png\n",
    "    | | |---image2.png\n",
    "    \n",
    "    :param base_dir: directories where image databse is stored\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    filenames = {}\n",
    "    filenames[\"Original\"] = []\n",
    "    filenames[\"Detection\"] = []\n",
    "    \n",
    "    # True if image is to be rotated counter-clockwise by 90 degrees.\n",
    "    filenames[\"Rotation\"] = []\n",
    "    \n",
    "    original_directory = os.path.join(base_dir,\"Original\")\n",
    "    detection_directory = os.path.join(base_dir,\"Detection\")\n",
    "    for file_name in os.listdir(original_directory):\n",
    "        if file_name.endswith(\".jpg\"):\n",
    "            original_name = os.path.join(original_directory, file_name)\n",
    "            base_filename=os.path.splitext(file_name)[0]\n",
    "            detection_name = os.path.join(detection_directory, base_filename + \".png\")\n",
    "            if os.path.exists(detection_name):\n",
    "                filenames[\"Rotation\"].append(True)\n",
    "                filenames[\"Original\"].append(original_name)\n",
    "                filenames[\"Detection\"].append(detection_name)\n",
    "                filenames[\"Rotation\"].append(False)\n",
    "                filenames[\"Original\"].append(original_name)\n",
    "                filenames[\"Detection\"].append(detection_name)\n",
    "        \n",
    "    return pd.DataFrame(filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames_dataset_1 = get_all_filenames(os.path.join(WORKPLACE_FOLDER, \"Dataset1\"))\n",
    "filenames_dataset_2 = get_all_filenames(os.path.join(WORKPLACE_FOLDER, \"Dataset2\"))\n",
    "filenames_dataset_3 = get_all_filenames(os.path.join(WORKPLACE_FOLDER, \"Dataset3\"))\n",
    "filenames_dataset_4 = get_all_filenames(os.path.join(WORKPLACE_FOLDER, \"Dataset4\"))\n",
    "filenames_dataset_5 = get_all_filenames(os.path.join(WORKPLACE_FOLDER, \"Dataset5\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(filenames_dataset_1.shape[0] == 244)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(filenames_dataset_2.shape[0] == 152)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(filenames_dataset_3.shape[0] == 122)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(filenames_dataset_4.shape[0] == 1726)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(filenames_dataset_5.shape[0] == 730)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation of images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(x):\n",
    "    # Normalize\n",
    "    x = tf.cast(x, tf.float32) / 255.0\n",
    "\n",
    "    # 'RGB'->'BGR'\n",
    "    x = x[..., ::-1]\n",
    "    return x\n",
    "\n",
    "def read_and_prepare_image(original_image_filename, mask_image_filename, is_rotated):\n",
    "    # Get images\n",
    "    original_image = tf.io.read_file(original_image_filename)\n",
    "    original_image_decoded = tf.image.decode_jpeg(original_image)\n",
    "    mask_image = tf.io.read_file(mask_image_filename)\n",
    "    mask_image_decoded = tf.image.decode_jpeg(mask_image)\n",
    "    \n",
    "    # Resize\n",
    "    original_image_resized = tf.image.resize(original_image_decoded, IMAGE_SIZE)\n",
    "    mask_image_resized = tf.image.resize(mask_image_decoded, IMAGE_SIZE)\n",
    "    \n",
    "    # Rotate\n",
    "    if is_rotated:\n",
    "        original_image_resized = tf.image.rot90(original_image_resized)\n",
    "        mask_image_resized = tf.image.rot90(mask_image_resized)\n",
    "\n",
    "    original_image_tensor = preprocess_image(original_image_resized)\n",
    "    mask_image_tensor = preprocess_image(mask_image_resized)\n",
    "    \n",
    "    return original_image_tensor, mask_image_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_tf_dataset(data, should_batch = True, should_repeat = True):\n",
    "    dataset_initial = tf.data.Dataset.from_tensor_slices((data.Original.values, data.Detection.values, data.Rotation.values))\n",
    "    dataset_mapped = dataset_initial.map(read_and_prepare_image)\n",
    "    dataset_shuffled = dataset_mapped.shuffle(buffer_size = len(data))\n",
    "    \n",
    "    if should_batch:\n",
    "        dataset = dataset_shuffled.batch(BATCH_SIZE)\n",
    "    else:\n",
    "        dataset = dataset_shuffled.batch(len(data))\n",
    "        \n",
    "    if should_repeat:\n",
    "        dataset = dataset.repeat()\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_1 = initialize_tf_dataset(filenames_dataset_4)\n",
    "train_data_2 = initialize_tf_dataset(filenames_dataset_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_map = {\n",
    "    train_data_1 : \"train1\",\n",
    "    train_data_2 : \"train2\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_1 = initialize_tf_dataset(filenames_dataset_1, should_batch = False, should_repeat = False)\n",
    "test_data_2 = initialize_tf_dataset(filenames_dataset_2, should_batch = False, should_repeat = False)\n",
    "test_data_3 = initialize_tf_dataset(filenames_dataset_3, should_batch = False, should_repeat = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_map = {\n",
    "    test_data_1 : \"test1\",\n",
    "    test_data_2 : \"test2\",\n",
    "    test_data_3 : \"test3\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data in train_data_map.keys():\n",
    "    for batch in data:\n",
    "        assert(batch[0].shape[0] == BATCH_SIZE)\n",
    "        assert(batch[0].shape[1] == INPUT_IMAGE_SIZE[0])\n",
    "        assert(batch[0].shape[2] == INPUT_IMAGE_SIZE[1])\n",
    "        assert(batch[0].shape[3] == INPUT_IMAGE_SIZE[2])\n",
    "        assert(batch[1].shape[0] == BATCH_SIZE)\n",
    "        assert(batch[1].shape[1] == INPUT_IMAGE_SIZE[0])\n",
    "        assert(batch[1].shape[2] == INPUT_IMAGE_SIZE[1])\n",
    "        assert(batch[1].shape[3] == 1)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data, filenames in zip(test_data_map.keys(),\n",
    "                           [filenames_dataset_1, filenames_dataset_2, filenames_dataset_3]):\n",
    "    for batch in data:\n",
    "        assert(batch[0].shape[0] == filenames.shape[0])\n",
    "        assert(batch[0].shape[1] == INPUT_IMAGE_SIZE[0])\n",
    "        assert(batch[0].shape[2] == INPUT_IMAGE_SIZE[1])\n",
    "        assert(batch[0].shape[3] == INPUT_IMAGE_SIZE[2])\n",
    "        assert(batch[1].shape[0] == filenames.shape[0])\n",
    "        assert(batch[1].shape[1] == INPUT_IMAGE_SIZE[0])\n",
    "        assert(batch[1].shape[2] == INPUT_IMAGE_SIZE[1])\n",
    "        assert(batch[1].shape[3] == 1)\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The models used the architecture inspired from SegNet [[4](#ref_4)] It is a convolutional neural network architecture applied for semantic pixel wise segmentation. It consists of an encoder, decoder and binary classification layers. Decoder maps the low resolution encoder result to input resolution size for pixel wise classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_2lw():\n",
    "    return Sequential([\n",
    "        Input(INPUT_IMAGE_SIZE),\n",
    "        Conv2D(8,(3,3), activation=tf.keras.activations.relu, padding='same'),\n",
    "        MaxPooling2D((2,2), padding='same'),\n",
    "        Conv2D(16,(3,3), activation=tf.keras.activations.relu, padding='same'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D((2,2), padding='same'),\n",
    "        Conv2D(16, (3, 3), activation=tf.keras.activations.relu, padding='same'),\n",
    "        UpSampling2D((2, 2)),\n",
    "        Conv2D(8, (3, 3), activation=tf.keras.activations.relu, padding='same'),\n",
    "        UpSampling2D((2, 2)),\n",
    "        Conv2D(1, (1, 1), activation=tf.keras.activations.relu, padding='same')\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_3lw():\n",
    "    return  Sequential([\n",
    "        Input(INPUT_IMAGE_SIZE),\n",
    "        Conv2D(8,(3,3), activation=tf.keras.activations.relu, padding='same'),\n",
    "        MaxPooling2D((2,2), padding='same'),\n",
    "        Conv2D(16,(3,3), activation=tf.keras.activations.relu, padding='same'),\n",
    "        MaxPooling2D((2,2), padding='same'),\n",
    "        Conv2D(32,(3,3), activation=tf.keras.activations.relu, padding='same'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D((2,2), padding='same'),\n",
    "        Conv2D(32, (3, 3), activation=tf.keras.activations.relu, padding='same'),\n",
    "        UpSampling2D((2, 2)),\n",
    "        Conv2D(16, (3, 3), activation=tf.keras.activations.relu, padding='same'),\n",
    "        UpSampling2D((2, 2)),\n",
    "        Conv2D(8, (3, 3), activation=tf.keras.activations.relu, padding='same'),\n",
    "        UpSampling2D((2, 2)),\n",
    "        Conv2D(1, (1, 1), activation=tf.keras.activations.relu, padding='same')\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_4lw():\n",
    "    return Sequential([\n",
    "        Input(INPUT_IMAGE_SIZE),\n",
    "        Conv2D(8,(3,3), activation=tf.keras.activations.relu, padding='same'),\n",
    "        MaxPooling2D((2,2), padding='same'),\n",
    "        Conv2D(16,(3,3), activation=tf.keras.activations.relu, padding='same'),\n",
    "        MaxPooling2D((2,2), padding='same'),\n",
    "        Conv2D(32,(3,3), activation=tf.keras.activations.relu, padding='same'),\n",
    "        MaxPooling2D((2,2), padding='same'),\n",
    "        Conv2D(64,(3,3), activation=tf.keras.activations.relu, padding='same'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D((2,2), padding='same'),\n",
    "        Conv2D(64, (3, 3), activation=tf.keras.activations.relu, padding='same'),\n",
    "        UpSampling2D((2, 2)),\n",
    "        Conv2D(32, (3, 3), activation=tf.keras.activations.relu, padding='same'),\n",
    "        UpSampling2D((2, 2)),\n",
    "        Conv2D(16, (3, 3), activation=tf.keras.activations.relu, padding='same'),\n",
    "        UpSampling2D((2, 2)),\n",
    "        Conv2D(8, (3, 3), activation=tf.keras.activations.relu, padding='same'),\n",
    "        UpSampling2D((2, 2)),\n",
    "        Conv2D(1, (1, 1), activation=tf.keras.activations.relu, padding='same')\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_w_map = {\n",
    "    get_model_2lw() : \"model2lw\",\n",
    "    get_model_3lw() : \"model3lw\",\n",
    "    get_model_4lw() : \"model4lw\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_2ln():\n",
    "    return Sequential([\n",
    "        Input(INPUT_IMAGE_SIZE),\n",
    "        Conv2D(4,(3,3), activation=tf.keras.activations.relu, padding='same'),\n",
    "        MaxPooling2D((2,2), padding='same'),\n",
    "        Conv2D(8,(3,3), activation=tf.keras.activations.relu, padding='same'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D((2,2), padding='same'),\n",
    "        Conv2D(8, (3, 3), activation=tf.keras.activations.relu, padding='same'),\n",
    "        UpSampling2D((2, 2)),\n",
    "        Conv2D(4, (3, 3), activation=tf.keras.activations.relu, padding='same'),\n",
    "        UpSampling2D((2, 2)),\n",
    "        Conv2D(1, (1, 1), activation=tf.keras.activations.relu, padding='same')\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_3ln():\n",
    "    return Sequential([\n",
    "        Input(INPUT_IMAGE_SIZE),\n",
    "        Conv2D(4,(3,3), activation=tf.keras.activations.relu, padding='same'),\n",
    "        MaxPooling2D((2,2), padding='same'),\n",
    "        Conv2D(8,(3,3), activation=tf.keras.activations.relu, padding='same'),\n",
    "        MaxPooling2D((2,2), padding='same'),\n",
    "        Conv2D(16,(3,3), activation=tf.keras.activations.relu, padding='same'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D((2,2), padding='same'),\n",
    "        Conv2D(16, (3, 3), activation=tf.keras.activations.relu, padding='same'),\n",
    "        UpSampling2D((2, 2)),\n",
    "        Conv2D(8, (3, 3), activation=tf.keras.activations.relu, padding='same'),\n",
    "        UpSampling2D((2, 2)),\n",
    "        Conv2D(4, (3, 3), activation=tf.keras.activations.relu, padding='same'),\n",
    "        UpSampling2D((2, 2)),\n",
    "        Conv2D(1, (1, 1), activation=tf.keras.activations.relu, padding='same')\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_4ln():\n",
    "    return Sequential([\n",
    "        Input(INPUT_IMAGE_SIZE),\n",
    "        Conv2D(4,(3,3), activation=tf.keras.activations.relu, padding='same'),\n",
    "        MaxPooling2D((2,2), padding='same'),\n",
    "        Conv2D(8,(3,3), activation=tf.keras.activations.relu, padding='same'),\n",
    "        MaxPooling2D((2,2), padding='same'),\n",
    "        Conv2D(16,(3,3), activation=tf.keras.activations.relu, padding='same'),\n",
    "        MaxPooling2D((2,2), padding='same'),\n",
    "        Conv2D(32,(3,3), activation=tf.keras.activations.relu, padding='same'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D((2,2), padding='same'),\n",
    "        Conv2D(32, (3, 3), activation=tf.keras.activations.relu, padding='same'),\n",
    "        UpSampling2D((2, 2)),\n",
    "        Conv2D(16, (3, 3), activation=tf.keras.activations.relu, padding='same'),\n",
    "        UpSampling2D((2, 2)),\n",
    "        Conv2D(8, (3, 3), activation=tf.keras.activations.relu, padding='same'),\n",
    "        UpSampling2D((2, 2)),\n",
    "        Conv2D(4, (3, 3), activation=tf.keras.activations.relu, padding='same'),\n",
    "        UpSampling2D((2, 2)),\n",
    "        Conv2D(1, (1, 1), activation=tf.keras.activations.relu, padding='same')\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_5ln():\n",
    "    return Sequential([\n",
    "        Input(INPUT_IMAGE_SIZE),\n",
    "        Conv2D(4,(3,3), activation=tf.keras.activations.relu, padding='same'),\n",
    "        MaxPooling2D((2,2), padding='same'),\n",
    "        Conv2D(8,(3,3), activation=tf.keras.activations.relu, padding='same'),\n",
    "        MaxPooling2D((2,2), padding='same'),\n",
    "        Conv2D(16,(3,3), activation=tf.keras.activations.relu, padding='same'),\n",
    "        MaxPooling2D((2,2), padding='same'),\n",
    "        Conv2D(32,(3,3), activation=tf.keras.activations.relu, padding='same'),\n",
    "        MaxPooling2D((2,2), padding='same'),\n",
    "        Conv2D(64,(3,3), activation=tf.keras.activations.relu, padding='same'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D((2,2), padding='same'),\n",
    "        Conv2D(64, (3, 3), activation=tf.keras.activations.relu, padding='same'),\n",
    "        UpSampling2D((2, 2)),\n",
    "        Conv2D(32, (3, 3), activation=tf.keras.activations.relu, padding='same'),\n",
    "        UpSampling2D((2, 2)),\n",
    "        Conv2D(16, (3, 3), activation=tf.keras.activations.relu, padding='same'),\n",
    "        UpSampling2D((2, 2)),\n",
    "        Conv2D(8, (3, 3), activation=tf.keras.activations.relu, padding='same'),\n",
    "        UpSampling2D((2, 2)),\n",
    "        Conv2D(4, (3, 3), activation=tf.keras.activations.relu, padding='same'),\n",
    "        UpSampling2D((2, 2)),\n",
    "        Conv2D(1, (1, 1), activation=tf.keras.activations.relu, padding='same')\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_n_map = {\n",
    "    get_model_2ln() : \"model2ln\",\n",
    "    get_model_3ln() : \"model3ln\",\n",
    "    get_model_4ln() : \"model4ln\",\n",
    "    get_model_5ln() : \"model5ln\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_44\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_352 (Conv2D)          (None, 224, 224, 8)       224       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_154 (MaxPoolin (None, 112, 112, 8)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_353 (Conv2D)          (None, 112, 112, 16)      1168      \n",
      "_________________________________________________________________\n",
      "batch_normalization_47 (Batc (None, 112, 112, 16)      64        \n",
      "_________________________________________________________________\n",
      "max_pooling2d_155 (MaxPoolin (None, 56, 56, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_354 (Conv2D)          (None, 56, 56, 16)        2320      \n",
      "_________________________________________________________________\n",
      "up_sampling2d_154 (UpSamplin (None, 112, 112, 16)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_355 (Conv2D)          (None, 112, 112, 8)       1160      \n",
      "_________________________________________________________________\n",
      "up_sampling2d_155 (UpSamplin (None, 224, 224, 8)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_356 (Conv2D)          (None, 224, 224, 1)       9         \n",
      "=================================================================\n",
      "Total params: 4,945\n",
      "Trainable params: 4,913\n",
      "Non-trainable params: 32\n",
      "_________________________________________________________________\n",
      "Model: \"sequential_45\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_357 (Conv2D)          (None, 224, 224, 8)       224       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_156 (MaxPoolin (None, 112, 112, 8)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_358 (Conv2D)          (None, 112, 112, 16)      1168      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_157 (MaxPoolin (None, 56, 56, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_359 (Conv2D)          (None, 56, 56, 32)        4640      \n",
      "_________________________________________________________________\n",
      "batch_normalization_48 (Batc (None, 56, 56, 32)        128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_158 (MaxPoolin (None, 28, 28, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_360 (Conv2D)          (None, 28, 28, 32)        9248      \n",
      "_________________________________________________________________\n",
      "up_sampling2d_156 (UpSamplin (None, 56, 56, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_361 (Conv2D)          (None, 56, 56, 16)        4624      \n",
      "_________________________________________________________________\n",
      "up_sampling2d_157 (UpSamplin (None, 112, 112, 16)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_362 (Conv2D)          (None, 112, 112, 8)       1160      \n",
      "_________________________________________________________________\n",
      "up_sampling2d_158 (UpSamplin (None, 224, 224, 8)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_363 (Conv2D)          (None, 224, 224, 1)       9         \n",
      "=================================================================\n",
      "Total params: 21,201\n",
      "Trainable params: 21,137\n",
      "Non-trainable params: 64\n",
      "_________________________________________________________________\n",
      "Model: \"sequential_46\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_364 (Conv2D)          (None, 224, 224, 8)       224       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_159 (MaxPoolin (None, 112, 112, 8)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_365 (Conv2D)          (None, 112, 112, 16)      1168      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_160 (MaxPoolin (None, 56, 56, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_366 (Conv2D)          (None, 56, 56, 32)        4640      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_161 (MaxPoolin (None, 28, 28, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_367 (Conv2D)          (None, 28, 28, 64)        18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_49 (Batc (None, 28, 28, 64)        256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_162 (MaxPoolin (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_368 (Conv2D)          (None, 14, 14, 64)        36928     \n",
      "_________________________________________________________________\n",
      "up_sampling2d_159 (UpSamplin (None, 28, 28, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_369 (Conv2D)          (None, 28, 28, 32)        18464     \n",
      "_________________________________________________________________\n",
      "up_sampling2d_160 (UpSamplin (None, 56, 56, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_370 (Conv2D)          (None, 56, 56, 16)        4624      \n",
      "_________________________________________________________________\n",
      "up_sampling2d_161 (UpSamplin (None, 112, 112, 16)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_371 (Conv2D)          (None, 112, 112, 8)       1160      \n",
      "_________________________________________________________________\n",
      "up_sampling2d_162 (UpSamplin (None, 224, 224, 8)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_372 (Conv2D)          (None, 224, 224, 1)       9         \n",
      "=================================================================\n",
      "Total params: 85,969\n",
      "Trainable params: 85,841\n",
      "Non-trainable params: 128\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "for model in model_w_map.keys():\n",
    "    model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_28\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_224 (Conv2D)          (None, 224, 224, 4)       112       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_98 (MaxPooling (None, 112, 112, 4)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_225 (Conv2D)          (None, 112, 112, 8)       296       \n",
      "_________________________________________________________________\n",
      "batch_normalization_28 (Batc (None, 112, 112, 8)       32        \n",
      "_________________________________________________________________\n",
      "max_pooling2d_99 (MaxPooling (None, 56, 56, 8)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_226 (Conv2D)          (None, 56, 56, 8)         584       \n",
      "_________________________________________________________________\n",
      "up_sampling2d_98 (UpSampling (None, 112, 112, 8)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_227 (Conv2D)          (None, 112, 112, 4)       292       \n",
      "_________________________________________________________________\n",
      "up_sampling2d_99 (UpSampling (None, 224, 224, 4)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_228 (Conv2D)          (None, 224, 224, 1)       5         \n",
      "=================================================================\n",
      "Total params: 1,321\n",
      "Trainable params: 1,305\n",
      "Non-trainable params: 16\n",
      "_________________________________________________________________\n",
      "Model: \"sequential_29\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_229 (Conv2D)          (None, 224, 224, 4)       112       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_100 (MaxPoolin (None, 112, 112, 4)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_230 (Conv2D)          (None, 112, 112, 8)       296       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_101 (MaxPoolin (None, 56, 56, 8)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_231 (Conv2D)          (None, 56, 56, 16)        1168      \n",
      "_________________________________________________________________\n",
      "batch_normalization_29 (Batc (None, 56, 56, 16)        64        \n",
      "_________________________________________________________________\n",
      "max_pooling2d_102 (MaxPoolin (None, 28, 28, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_232 (Conv2D)          (None, 28, 28, 16)        2320      \n",
      "_________________________________________________________________\n",
      "up_sampling2d_100 (UpSamplin (None, 56, 56, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_233 (Conv2D)          (None, 56, 56, 8)         1160      \n",
      "_________________________________________________________________\n",
      "up_sampling2d_101 (UpSamplin (None, 112, 112, 8)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_234 (Conv2D)          (None, 112, 112, 4)       292       \n",
      "_________________________________________________________________\n",
      "up_sampling2d_102 (UpSamplin (None, 224, 224, 4)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_235 (Conv2D)          (None, 224, 224, 1)       5         \n",
      "=================================================================\n",
      "Total params: 5,417\n",
      "Trainable params: 5,385\n",
      "Non-trainable params: 32\n",
      "_________________________________________________________________\n",
      "Model: \"sequential_30\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_236 (Conv2D)          (None, 224, 224, 4)       112       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_103 (MaxPoolin (None, 112, 112, 4)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_237 (Conv2D)          (None, 112, 112, 8)       296       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_104 (MaxPoolin (None, 56, 56, 8)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_238 (Conv2D)          (None, 56, 56, 16)        1168      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_105 (MaxPoolin (None, 28, 28, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_239 (Conv2D)          (None, 28, 28, 32)        4640      \n",
      "_________________________________________________________________\n",
      "batch_normalization_30 (Batc (None, 28, 28, 32)        128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_106 (MaxPoolin (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_240 (Conv2D)          (None, 14, 14, 32)        9248      \n",
      "_________________________________________________________________\n",
      "up_sampling2d_103 (UpSamplin (None, 28, 28, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_241 (Conv2D)          (None, 28, 28, 16)        4624      \n",
      "_________________________________________________________________\n",
      "up_sampling2d_104 (UpSamplin (None, 56, 56, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_242 (Conv2D)          (None, 56, 56, 8)         1160      \n",
      "_________________________________________________________________\n",
      "up_sampling2d_105 (UpSamplin (None, 112, 112, 8)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_243 (Conv2D)          (None, 112, 112, 4)       292       \n",
      "_________________________________________________________________\n",
      "up_sampling2d_106 (UpSamplin (None, 224, 224, 4)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_244 (Conv2D)          (None, 224, 224, 1)       5         \n",
      "=================================================================\n",
      "Total params: 21,673\n",
      "Trainable params: 21,609\n",
      "Non-trainable params: 64\n",
      "_________________________________________________________________\n",
      "Model: \"sequential_31\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_245 (Conv2D)          (None, 224, 224, 4)       112       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_107 (MaxPoolin (None, 112, 112, 4)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_246 (Conv2D)          (None, 112, 112, 8)       296       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_108 (MaxPoolin (None, 56, 56, 8)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_247 (Conv2D)          (None, 56, 56, 16)        1168      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_109 (MaxPoolin (None, 28, 28, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_248 (Conv2D)          (None, 28, 28, 32)        4640      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_110 (MaxPoolin (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_249 (Conv2D)          (None, 14, 14, 64)        18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_31 (Batc (None, 14, 14, 64)        256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_111 (MaxPoolin (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_250 (Conv2D)          (None, 7, 7, 64)          36928     \n",
      "_________________________________________________________________\n",
      "up_sampling2d_107 (UpSamplin (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_251 (Conv2D)          (None, 14, 14, 32)        18464     \n",
      "_________________________________________________________________\n",
      "up_sampling2d_108 (UpSamplin (None, 28, 28, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_252 (Conv2D)          (None, 28, 28, 16)        4624      \n",
      "_________________________________________________________________\n",
      "up_sampling2d_109 (UpSamplin (None, 56, 56, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_253 (Conv2D)          (None, 56, 56, 8)         1160      \n",
      "_________________________________________________________________\n",
      "up_sampling2d_110 (UpSamplin (None, 112, 112, 8)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_254 (Conv2D)          (None, 112, 112, 4)       292       \n",
      "_________________________________________________________________\n",
      "up_sampling2d_111 (UpSamplin (None, 224, 224, 4)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_255 (Conv2D)          (None, 224, 224, 1)       5         \n",
      "=================================================================\n",
      "Total params: 86,441\n",
      "Trainable params: 86,313\n",
      "Non-trainable params: 128\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "for model in model_n_map.keys():\n",
    "    model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_per_epoch_train_1 = round(len(filenames_dataset_4) * 1.0 / BATCH_SIZE)\n",
    "steps_per_epoch_train_2 = round(len(filenames_dataset_5) * 1.0 / BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "steps_per_epoch_train_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "steps_per_epoch_train_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_per_epoch_map = {\n",
    "    train_data_1 : steps_per_epoch_train_1,\n",
    "    train_data_2 : steps_per_epoch_train_2\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(CHECKPOINTS_DIR):\n",
    "    os.makedirs(CHECKPOINTS_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoints/train1_model2lw\n",
      "54/54 [==============================] - 11s 201ms/step - loss: 0.0059\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.0681\n",
      "1/1 [==============================] - 5s 5s/step - loss: 0.0378\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0176\n",
      "checkpoints/train1_model3lw\n",
      "54/54 [==============================] - 11s 204ms/step - loss: 0.0044\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.0619\n",
      "1/1 [==============================] - 5s 5s/step - loss: 0.0282\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0160\n",
      "checkpoints/train1_model4lw\n",
      "54/54 [==============================] - 12s 213ms/step - loss: 0.0030\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.0576\n",
      "1/1 [==============================] - 5s 5s/step - loss: 0.0284\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0170\n",
      "checkpoints/train1_model2ln\n",
      "54/54 [==============================] - 10s 180ms/step - loss: 0.0085\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.0655\n",
      "1/1 [==============================] - 5s 5s/step - loss: 0.0284\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0160\n",
      "checkpoints/train1_model3ln\n",
      "54/54 [==============================] - 10s 186ms/step - loss: 0.0055\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.0614\n",
      "1/1 [==============================] - 5s 5s/step - loss: 0.0285\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0162\n",
      "checkpoints/train1_model4ln\n",
      "54/54 [==============================] - 10s 189ms/step - loss: 0.0045\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.0475\n",
      "1/1 [==============================] - 5s 5s/step - loss: 0.0239\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0150\n",
      "checkpoints/train1_model5ln\n",
      "54/54 [==============================] - 10s 191ms/step - loss: 0.0026\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.0343\n",
      "1/1 [==============================] - 5s 5s/step - loss: 0.0213\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0131\n",
      "checkpoints/train2_model2lw\n",
      "23/23 [==============================] - 5s 198ms/step - loss: 0.0099\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.0155\n",
      "1/1 [==============================] - 5s 5s/step - loss: 0.0137\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0063\n",
      "checkpoints/train2_model3lw\n",
      "23/23 [==============================] - 5s 211ms/step - loss: 0.0062\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.0141\n",
      "1/1 [==============================] - 5s 5s/step - loss: 0.0131\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0076\n",
      "checkpoints/train2_model4lw\n",
      "23/23 [==============================] - 5s 220ms/step - loss: 0.0031\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.0115\n",
      "1/1 [==============================] - 5s 5s/step - loss: 0.0121\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0073\n",
      "checkpoints/train2_model2ln\n",
      "23/23 [==============================] - 4s 186ms/step - loss: 0.0225\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.0249\n",
      "1/1 [==============================] - 5s 5s/step - loss: 0.0232\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0094\n",
      "checkpoints/train2_model3ln\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/alpha/guide/checkpoints#loading_mechanics for details.\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/alpha/guide/checkpoints#loading_mechanics for details.\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/alpha/guide/checkpoints#loading_mechanics for details.\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/alpha/guide/checkpoints#loading_mechanics for details.\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/alpha/guide/checkpoints#loading_mechanics for details.\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/alpha/guide/checkpoints#loading_mechanics for details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/alpha/guide/checkpoints#loading_mechanics for details.\n",
      "23/23 [==============================] - 5s 210ms/step - loss: 0.0073\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.0151\n",
      "1/1 [==============================] - 5s 5s/step - loss: 0.0138\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0061\n",
      "checkpoints/train2_model4ln\n",
      "23/23 [==============================] - 5s 199ms/step - loss: 0.0041\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.0125\n",
      "1/1 [==============================] - 5s 5s/step - loss: 0.0122\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0072\n",
      "checkpoints/train2_model5ln\n",
      "23/23 [==============================] - 5s 212ms/step - loss: 0.0039\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.0135\n",
      "1/1 [==============================] - 5s 5s/step - loss: 0.0127\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0119\n"
     ]
    }
   ],
   "source": [
    "fitted_models = {}\n",
    "train_evaluations = {}\n",
    "test_evaluations = {}\n",
    "\n",
    "for train_item in train_data_map.items():\n",
    "    train_data = train_item[0]\n",
    "    train_name = train_item[1]\n",
    "    \n",
    "    fitted_models_train_set = {}\n",
    "    test_evaluations_train_set = {}\n",
    "    train_evaluations_train_set = {}\n",
    "    \n",
    "    for model_item in {**model_w_map , **model_n_map}.items():\n",
    "        model_name = model_item[1]\n",
    "        \n",
    "        model = clone_model(model_item[0])\n",
    "        \n",
    "        checkpoint_basename = train_name + \"_\" + model_name\n",
    "        checkpoint_path = os.path.join(CHECKPOINTS_DIR,checkpoint_basename)\n",
    "        \n",
    "        model_weights_file = os.path.join(checkpoint_path, \"cp.ckpt\")\n",
    "        log_path = os.path.join(checkpoint_path, \"log\")\n",
    "        \n",
    "        # Callback to save the model's weights\n",
    "        cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=model_weights_file,\n",
    "                                                         save_weights_only=True)  \n",
    "\n",
    "        print(checkpoint_path)\n",
    "        \n",
    "        needs_training = True\n",
    "        if os.path.exists(checkpoint_path) and RESUME_MODELS:\n",
    "            model.load_weights(model_weights_file).expect_partial()\n",
    "            needs_training = False            \n",
    "        \n",
    "        model.compile(optimizer=Adam(), loss=MeanSquaredError())\n",
    "        \n",
    "        if TRAIN_MODELS or needs_training:\n",
    "            history = model.fit(train_data,\n",
    "                                epochs = NUM_EPOCHS,\n",
    "                                steps_per_epoch = steps_per_epoch_map[train_data],\n",
    "                                callbacks = [TensorBoard(log_dir=log_path),\n",
    "                                             cp_callback]\n",
    "                               )\n",
    "        \n",
    "        train_evaluations_train_set[model_item] = \\\n",
    "        model.evaluate(train_data, steps = steps_per_epoch_map[train_data])\n",
    "        \n",
    "        test_evaluations_test_set = {}\n",
    "        for test_item in test_data_map.items():\n",
    "            test_data = test_item[0]\n",
    "            test_evaluations_test_set[test_data] = model.evaluate(test_data)\n",
    "        \n",
    "        fitted_models_train_set[model_item] = model\n",
    "        test_evaluations_train_set[model_item] = test_evaluations_test_set\n",
    "        \n",
    "    fitted_models[train_data] = fitted_models_train_set\n",
    "    test_evaluations[train_data] = test_evaluations_train_set\n",
    "    train_evaluations[train_data] = train_evaluations_train_set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train1\n",
      "model2lw\n",
      "54/54 [==============================] - 11s 207ms/step - loss: 0.0059\n",
      "model3lw\n",
      "54/54 [==============================] - 11s 203ms/step - loss: 0.0044\n",
      "model4lw\n",
      "54/54 [==============================] - 12s 230ms/step - loss: 0.0030\n",
      "\n",
      "model2ln\n",
      "54/54 [==============================] - 10s 185ms/step - loss: 0.0085\n",
      "model3ln\n",
      "54/54 [==============================] - 10s 188ms/step - loss: 0.0055\n",
      "model4ln\n",
      "54/54 [==============================] - 10s 192ms/step - loss: 0.0045\n",
      "model5ln\n",
      "54/54 [==============================] - 10s 194ms/step - loss: 0.0026\n",
      "\n",
      "\n",
      "train2\n",
      "model2lw\n",
      "23/23 [==============================] - 5s 203ms/step - loss: 0.0099\n",
      "model3lw\n",
      "23/23 [==============================] - 5s 211ms/step - loss: 0.0062\n",
      "model4lw\n",
      "23/23 [==============================] - 5s 225ms/step - loss: 0.0031\n",
      "\n",
      "model2ln\n",
      "23/23 [==============================] - 4s 182ms/step - loss: 0.0224\n",
      "model3ln\n",
      "23/23 [==============================] - 5s 196ms/step - loss: 0.0073\n",
      "model4ln\n",
      "23/23 [==============================] - 5s 199ms/step - loss: 0.0041\n",
      "model5ln\n",
      "23/23 [==============================] - 5s 200ms/step - loss: 0.0039\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for train_item in train_data_map.items():\n",
    "    train_data = train_item[0]\n",
    "    print(train_item[1])\n",
    "    for model_item in model_w_map.items():\n",
    "        print(model_item[1])\n",
    "        model = fitted_models[train_data][model_item]\n",
    "        model.evaluate(train_data, steps = steps_per_epoch_map[train_data])\n",
    "    print()\n",
    "    for model_item in model_n_map.items():\n",
    "        print(model_item[1])\n",
    "        model = fitted_models[train_data][model_item]\n",
    "        model.evaluate(train_data, steps = steps_per_epoch_map[train_data])\n",
    "    print()     \n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_test_scores_map = {\n",
    "    test_data_1 : \"Test Score 1\",\n",
    "    test_data_2 : \"Test Score 2\",\n",
    "    test_data_3 : \"Test Score 3\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_scores(train_data, model_map):\n",
    "    model_scores = {}\n",
    "    model_scores[\"Model\"] = []\n",
    "    model_scores[\"Train Score\"] = []\n",
    "    model_scores[\"Test Score 1\"] = []\n",
    "    model_scores[\"Test Score 2\"] = []\n",
    "    model_scores[\"Test Score 3\"] = []\n",
    "\n",
    "    for model_item in model_map.items():\n",
    "        model_scores[\"Model\"].append(model_item[1])\n",
    "        model_scores[\"Train Score\"].append(train_evaluations[train_data][model_item])\n",
    "        for test_item in test_data_map.items():\n",
    "            test_data = test_item[0]\n",
    "            evaluation = test_evaluations[train_data][model_item][test_data]\n",
    "            model_scores[model_test_scores_map[test_data]].append(evaluation)     \n",
    "    \n",
    "    return pd.DataFrame(model_scores)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Train Score</th>\n",
       "      <th>Test Score 1</th>\n",
       "      <th>Test Score 2</th>\n",
       "      <th>Test Score 3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>model2lw</td>\n",
       "      <td>0.005942</td>\n",
       "      <td>0.068149</td>\n",
       "      <td>0.037784</td>\n",
       "      <td>0.017575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>model3lw</td>\n",
       "      <td>0.004435</td>\n",
       "      <td>0.061945</td>\n",
       "      <td>0.028210</td>\n",
       "      <td>0.015983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>model4lw</td>\n",
       "      <td>0.003046</td>\n",
       "      <td>0.057595</td>\n",
       "      <td>0.028425</td>\n",
       "      <td>0.016955</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Model  Train Score  Test Score 1  Test Score 2  Test Score 3\n",
       "0  model2lw     0.005942      0.068149      0.037784      0.017575\n",
       "1  model3lw     0.004435      0.061945      0.028210      0.015983\n",
       "2  model4lw     0.003046      0.057595      0.028425      0.016955"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_model_scores(train_data_1, model_w_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Train Score</th>\n",
       "      <th>Test Score 1</th>\n",
       "      <th>Test Score 2</th>\n",
       "      <th>Test Score 3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>model2ln</td>\n",
       "      <td>0.008528</td>\n",
       "      <td>0.065537</td>\n",
       "      <td>0.028444</td>\n",
       "      <td>0.015953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>model3ln</td>\n",
       "      <td>0.005502</td>\n",
       "      <td>0.061433</td>\n",
       "      <td>0.028468</td>\n",
       "      <td>0.016208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>model4ln</td>\n",
       "      <td>0.004515</td>\n",
       "      <td>0.047457</td>\n",
       "      <td>0.023913</td>\n",
       "      <td>0.015039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>model5ln</td>\n",
       "      <td>0.002647</td>\n",
       "      <td>0.034251</td>\n",
       "      <td>0.021288</td>\n",
       "      <td>0.013139</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Model  Train Score  Test Score 1  Test Score 2  Test Score 3\n",
       "0  model2ln     0.008528      0.065537      0.028444      0.015953\n",
       "1  model3ln     0.005502      0.061433      0.028468      0.016208\n",
       "2  model4ln     0.004515      0.047457      0.023913      0.015039\n",
       "3  model5ln     0.002647      0.034251      0.021288      0.013139"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_model_scores(train_data_1, model_n_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Train Score</th>\n",
       "      <th>Test Score 1</th>\n",
       "      <th>Test Score 2</th>\n",
       "      <th>Test Score 3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>model2lw</td>\n",
       "      <td>0.009834</td>\n",
       "      <td>0.015484</td>\n",
       "      <td>0.013720</td>\n",
       "      <td>0.006260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>model3lw</td>\n",
       "      <td>0.006165</td>\n",
       "      <td>0.014134</td>\n",
       "      <td>0.013086</td>\n",
       "      <td>0.007609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>model4lw</td>\n",
       "      <td>0.003055</td>\n",
       "      <td>0.011455</td>\n",
       "      <td>0.012116</td>\n",
       "      <td>0.007313</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Model  Train Score  Test Score 1  Test Score 2  Test Score 3\n",
       "0  model2lw     0.009834      0.015484      0.013720      0.006260\n",
       "1  model3lw     0.006165      0.014134      0.013086      0.007609\n",
       "2  model4lw     0.003055      0.011455      0.012116      0.007313"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_model_scores(train_data_2, model_w_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Train Score</th>\n",
       "      <th>Test Score 1</th>\n",
       "      <th>Test Score 2</th>\n",
       "      <th>Test Score 3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>model2ln</td>\n",
       "      <td>0.008528</td>\n",
       "      <td>0.065537</td>\n",
       "      <td>0.028444</td>\n",
       "      <td>0.015953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>model3ln</td>\n",
       "      <td>0.005502</td>\n",
       "      <td>0.061433</td>\n",
       "      <td>0.028468</td>\n",
       "      <td>0.016208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>model4ln</td>\n",
       "      <td>0.004515</td>\n",
       "      <td>0.047457</td>\n",
       "      <td>0.023913</td>\n",
       "      <td>0.015039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>model5ln</td>\n",
       "      <td>0.002647</td>\n",
       "      <td>0.034251</td>\n",
       "      <td>0.021288</td>\n",
       "      <td>0.013139</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Model  Train Score  Test Score 1  Test Score 2  Test Score 3\n",
       "0  model2ln     0.008528      0.065537      0.028444      0.015953\n",
       "1  model3ln     0.005502      0.061433      0.028468      0.016208\n",
       "2  model4ln     0.004515      0.047457      0.023913      0.015039\n",
       "3  model5ln     0.002647      0.034251      0.021288      0.013139"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_model_scores(train_data_1, model_n_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0026470055777786505"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.concat({\"t1\":pd.concat([create_model_scores(train_data_1, model_w_map),\n",
    "                 create_model_scores(train_data_1, model_n_map)]),\n",
    "           \"t2\": pd.concat([create_model_scores(train_data_2, model_w_map),\n",
    "                  create_model_scores(train_data_2, model_n_map)])})[\"Train Score\"].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Test Score 1    0.011455\n",
       "Test Score 2    0.012116\n",
       "Test Score 3    0.006137\n",
       "dtype: float64"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.concat({\"t1\":pd.concat([create_model_scores(train_data_1, model_w_map),\n",
    "                 create_model_scores(train_data_1, model_n_map)]),\n",
    "           \"t2\": pd.concat([create_model_scores(train_data_2, model_w_map),\n",
    "                  create_model_scores(train_data_2, model_n_map)])})[[\"Test Score 1\", \"Test Score 2\", \"Test Score 3\"]].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('t1', 3)"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.concat({\"t1\":pd.concat([create_model_scores(train_data_1, model_w_map),\n",
    "                 create_model_scores(train_data_1, model_n_map)]),\n",
    "           \"t2\": pd.concat([create_model_scores(train_data_2, model_w_map),\n",
    "                  create_model_scores(train_data_2, model_n_map)])})[\"Train Score\"].idxmin()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Test Score 1    (t2, 2)\n",
       "Test Score 2    (t2, 2)\n",
       "Test Score 3    (t2, 1)\n",
       "dtype: object"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.concat({\"t1\":pd.concat([create_model_scores(train_data_1, model_w_map),\n",
    "                 create_model_scores(train_data_1, model_n_map)]),\n",
    "           \"t2\": pd.concat([create_model_scores(train_data_2, model_w_map),\n",
    "                  create_model_scores(train_data_2, model_n_map)])})[[\"Test Score 1\", \"Test Score 2\", \"Test Score 3\"]].idxmin()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There should be a function to create masks and compare masks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Followed by some notable examples of failure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Training with different barcode orientation improves segmentation accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='ref_1'></a>\n",
    "1. Alessandro Zamberletti, Ignazio Gallo, Moreno Carullo and Elisabetta Binaghi \"Neural Image Restoration For Decoding 1-D Barcodes Using Common Camera Phones\" Computer Vision, Imaging and Computer Graphics. Theory and Applications, Springer Berlin Heidelberg, 2011"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='ref_2'></a>\n",
    "2. S. Wachenfeld, S. Terlunen, X.Jiang  \"Robust recognition of 1-d barcodes using camera phones.\"\"\n",
    "In Proceedings of the 2008 19th International Conference on Pattern Recognition, Tampa, FL, USA,\n",
    "8–11 December 2008; pp. 1–4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='ref_3'></a>\n",
    "3. Alessandro Zamberletti, Ignazio Gallo and Simone Albertini \"Robust Angle Invariant 1D Barcode Detection\" Proceedings of the 2nd Asian Conference on Pattern Recognition (ACPR), Okinawa, Japan, 2013"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='ref_4'></a>\n",
    "4. V. Badrinarayanan, A. Kendall and R. Cipolla, \"SegNet: A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation,\" in IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 39, no. 12, pp. 2481-2495, 1 Dec. 2017."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
