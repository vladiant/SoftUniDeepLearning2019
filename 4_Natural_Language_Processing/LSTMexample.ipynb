{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://machinelearningmastery.com/tensorflow-tutorial-deep-learning-with-tf-keras/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split univariate sequence into samples\n",
    "def split_sequence(sequence, n_steps):\n",
    "    X, y = list(), list()\n",
    "    for i in range(len(sequence)):\n",
    "        # Find the end of this pattern\n",
    "        end_ix = i + n_steps\n",
    "        # Check if we are beyond the sequence\n",
    "        if end_ix > len(sequence) - 1:\n",
    "            break\n",
    "        # Gather input and output patterns of the pattern\n",
    "        seq_x, seq_y = sequence[i:end_ix], sequence[end_ix]\n",
    "        X.append(seq_x)\n",
    "        y.append(seq_y)\n",
    "    return np.asarray(X), np.asarray(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "path = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/monthly-car-sales.csv'\n",
    "df = pd.read_csv(path, header=0, index_col=0, squeeze=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the values\n",
    "values = df.values.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the window size\n",
    "n_steps = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into samples\n",
    "X, y = split_sequence(values, n_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape into [samples, timesteps, features]\n",
    "X = X.reshape((X.shape[0], X.shape[1], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(91, 5, 1) (12, 5, 1) (91,) (12,)\n"
     ]
    }
   ],
   "source": [
    "# Split into train/test\n",
    "n_test = 12\n",
    "X_train, X_test, y_train, y_test = X[:-n_test], X[-n_test:], y[:-n_test], y[-n_test:]\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model\n",
    "model = Sequential()\n",
    "model.add(LSTM(100, activation='relu', kernel_initializer='he_normal', input_shape=(n_steps,1)))\n",
    "model.add(Dense(50, activation='relu', kernel_initializer='he_normal'))\n",
    "model.add(Dense(50, activation='relu', kernel_initializer='he_normal'))\n",
    "model.add(Dense(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 91 samples, validate on 12 samples\n",
      "Epoch 1/350\n",
      "91/91 - 1s - loss: 153889756.2198 - mae: 9995.7432 - val_loss: 98318824.0000 - val_mae: 9279.7471\n",
      "Epoch 2/350\n",
      "91/91 - 0s - loss: 102124277.8901 - mae: 8456.7002 - val_loss: 108234752.0000 - val_mae: 9200.3398\n",
      "Epoch 3/350\n",
      "91/91 - 0s - loss: 63319468.7912 - mae: 6681.8330 - val_loss: 41852652.0000 - val_mae: 5919.1030\n",
      "Epoch 4/350\n",
      "91/91 - 0s - loss: 33251845.4066 - mae: 4504.6699 - val_loss: 45362316.0000 - val_mae: 5076.3784\n",
      "Epoch 5/350\n",
      "91/91 - 0s - loss: 31650054.1978 - mae: 4386.0396 - val_loss: 36074276.0000 - val_mae: 4548.7622\n",
      "Epoch 6/350\n",
      "91/91 - 0s - loss: 26072163.6923 - mae: 4013.6326 - val_loss: 27616752.0000 - val_mae: 4267.7476\n",
      "Epoch 7/350\n",
      "91/91 - 0s - loss: 28157550.1099 - mae: 4112.1807 - val_loss: 35488264.0000 - val_mae: 5060.0103\n",
      "Epoch 8/350\n",
      "91/91 - 0s - loss: 34887381.6703 - mae: 4374.2808 - val_loss: 25271982.0000 - val_mae: 4013.6946\n",
      "Epoch 9/350\n",
      "91/91 - 0s - loss: 28837707.5604 - mae: 4015.5530 - val_loss: 73098856.0000 - val_mae: 5751.2930\n",
      "Epoch 10/350\n",
      "91/91 - 0s - loss: 30577592.7912 - mae: 4393.1196 - val_loss: 57770196.0000 - val_mae: 5100.0835\n",
      "Epoch 11/350\n",
      "91/91 - 0s - loss: 22014904.8901 - mae: 3769.0981 - val_loss: 44068948.0000 - val_mae: 4880.2383\n",
      "Epoch 12/350\n",
      "91/91 - 0s - loss: 23645197.5165 - mae: 3896.3875 - val_loss: 37412944.0000 - val_mae: 4789.6099\n",
      "Epoch 13/350\n",
      "91/91 - 0s - loss: 23279518.3846 - mae: 3864.3420 - val_loss: 32683568.0000 - val_mae: 4747.4624\n",
      "Epoch 14/350\n",
      "91/91 - 0s - loss: 18423781.3846 - mae: 3386.4595 - val_loss: 28824822.0000 - val_mae: 4601.0591\n",
      "Epoch 15/350\n",
      "91/91 - 0s - loss: 14897714.1978 - mae: 2996.2324 - val_loss: 26850490.0000 - val_mae: 4287.9238\n",
      "Epoch 16/350\n",
      "91/91 - 0s - loss: 16517219.4505 - mae: 2968.2634 - val_loss: 20966218.0000 - val_mae: 3534.4250\n",
      "Epoch 17/350\n",
      "91/91 - 0s - loss: 14964266.6813 - mae: 2867.5947 - val_loss: 18042406.0000 - val_mae: 3280.8879\n",
      "Epoch 18/350\n",
      "91/91 - 0s - loss: 14415038.6484 - mae: 2752.6165 - val_loss: 22149618.0000 - val_mae: 3722.0203\n",
      "Epoch 19/350\n",
      "91/91 - 0s - loss: 12217185.0000 - mae: 2559.1548 - val_loss: 26911630.0000 - val_mae: 4152.8345\n",
      "Epoch 20/350\n",
      "91/91 - 0s - loss: 12768809.9451 - mae: 2547.6621 - val_loss: 33146224.0000 - val_mae: 4864.5688\n",
      "Epoch 21/350\n",
      "91/91 - 0s - loss: 13102372.6374 - mae: 2681.6802 - val_loss: 30829928.0000 - val_mae: 4603.4604\n",
      "Epoch 22/350\n",
      "91/91 - 0s - loss: 13139624.2198 - mae: 2712.8059 - val_loss: 24390182.0000 - val_mae: 4039.8816\n",
      "Epoch 23/350\n",
      "91/91 - 0s - loss: 13634189.3187 - mae: 2888.9124 - val_loss: 22119456.0000 - val_mae: 3630.0696\n",
      "Epoch 24/350\n",
      "91/91 - 0s - loss: 11184101.6813 - mae: 2657.2012 - val_loss: 21465592.0000 - val_mae: 3743.0642\n",
      "Epoch 25/350\n",
      "91/91 - 0s - loss: 11408995.0549 - mae: 2649.1904 - val_loss: 17822242.0000 - val_mae: 3430.8723\n",
      "Epoch 26/350\n",
      "91/91 - 0s - loss: 11115921.6154 - mae: 2607.5803 - val_loss: 18707526.0000 - val_mae: 3678.5088\n",
      "Epoch 27/350\n",
      "91/91 - 0s - loss: 10604804.3516 - mae: 2466.3169 - val_loss: 18713256.0000 - val_mae: 3762.5176\n",
      "Epoch 28/350\n",
      "91/91 - 0s - loss: 10492964.8681 - mae: 2446.8364 - val_loss: 14391444.0000 - val_mae: 3215.7766\n",
      "Epoch 29/350\n",
      "91/91 - 0s - loss: 10309666.3736 - mae: 2469.3354 - val_loss: 15381784.0000 - val_mae: 3329.3447\n",
      "Epoch 30/350\n",
      "91/91 - 0s - loss: 9852953.7692 - mae: 2456.2407 - val_loss: 14561917.0000 - val_mae: 2893.4285\n",
      "Epoch 31/350\n",
      "91/91 - 0s - loss: 10740697.0659 - mae: 2526.3030 - val_loss: 14139049.0000 - val_mae: 2864.2874\n",
      "Epoch 32/350\n",
      "91/91 - 0s - loss: 30655144.0879 - mae: 3320.7815 - val_loss: 14773973.0000 - val_mae: 2873.6963\n",
      "Epoch 33/350\n",
      "91/91 - 0s - loss: 35039790.9011 - mae: 3774.8708 - val_loss: 42038952.0000 - val_mae: 4425.0972\n",
      "Epoch 34/350\n",
      "91/91 - 0s - loss: 38020675.2088 - mae: 4334.4917 - val_loss: 36070176.0000 - val_mae: 4781.2417\n",
      "Epoch 35/350\n",
      "91/91 - 0s - loss: 33547578.5275 - mae: 4459.4517 - val_loss: 67046604.0000 - val_mae: 5724.4536\n",
      "Epoch 36/350\n",
      "91/91 - 0s - loss: 30701275.6703 - mae: 4277.7690 - val_loss: 55502736.0000 - val_mae: 5063.3652\n",
      "Epoch 37/350\n",
      "91/91 - 0s - loss: 23655097.8022 - mae: 3768.0994 - val_loss: 43827692.0000 - val_mae: 4529.0239\n",
      "Epoch 38/350\n",
      "91/91 - 0s - loss: 21266171.5275 - mae: 3528.6562 - val_loss: 21649412.0000 - val_mae: 3007.9207\n",
      "Epoch 39/350\n",
      "91/91 - 0s - loss: 15742719.6703 - mae: 2979.5811 - val_loss: 14640576.0000 - val_mae: 2726.0427\n",
      "Epoch 40/350\n",
      "91/91 - 0s - loss: 17558987.1868 - mae: 3230.6548 - val_loss: 11341836.0000 - val_mae: 2692.0825\n",
      "Epoch 41/350\n",
      "91/91 - 0s - loss: 23151575.4725 - mae: 3657.1665 - val_loss: 16368195.0000 - val_mae: 3068.7559\n",
      "Epoch 42/350\n",
      "91/91 - 0s - loss: 16258933.2418 - mae: 3177.6292 - val_loss: 29489008.0000 - val_mae: 3979.6846\n",
      "Epoch 43/350\n",
      "91/91 - 0s - loss: 21490993.0110 - mae: 3680.9863 - val_loss: 53813548.0000 - val_mae: 5307.3970\n",
      "Epoch 44/350\n",
      "91/91 - 0s - loss: 29521029.2088 - mae: 4221.4175 - val_loss: 20424808.0000 - val_mae: 3306.2698\n",
      "Epoch 45/350\n",
      "91/91 - 0s - loss: 22420396.0659 - mae: 3636.5193 - val_loss: 27269622.0000 - val_mae: 4442.0688\n",
      "Epoch 46/350\n",
      "91/91 - 0s - loss: 20251536.7912 - mae: 3446.1479 - val_loss: 16738665.0000 - val_mae: 3455.5554\n",
      "Epoch 47/350\n",
      "91/91 - 0s - loss: 16194392.4835 - mae: 3243.4966 - val_loss: 13818163.0000 - val_mae: 3047.3484\n",
      "Epoch 48/350\n",
      "91/91 - 0s - loss: 13778445.0879 - mae: 3019.2561 - val_loss: 25909088.0000 - val_mae: 4100.1011\n",
      "Epoch 49/350\n",
      "91/91 - 0s - loss: 17444426.1978 - mae: 3338.5508 - val_loss: 22129678.0000 - val_mae: 3848.9172\n",
      "Epoch 50/350\n",
      "91/91 - 0s - loss: 15292469.4066 - mae: 3142.7473 - val_loss: 18199374.0000 - val_mae: 3657.1960\n",
      "Epoch 51/350\n",
      "91/91 - 0s - loss: 12687112.8462 - mae: 2884.2888 - val_loss: 23998038.0000 - val_mae: 4449.6958\n",
      "Epoch 52/350\n",
      "91/91 - 0s - loss: 15374104.4725 - mae: 3093.7874 - val_loss: 19675966.0000 - val_mae: 3817.0750\n",
      "Epoch 53/350\n",
      "91/91 - 0s - loss: 13531934.3297 - mae: 2853.9463 - val_loss: 17699390.0000 - val_mae: 3367.5261\n",
      "Epoch 54/350\n",
      "91/91 - 0s - loss: 11460676.4286 - mae: 2742.8098 - val_loss: 13771864.0000 - val_mae: 3005.0085\n",
      "Epoch 55/350\n",
      "91/91 - 0s - loss: 11858983.2967 - mae: 2724.0522 - val_loss: 18618458.0000 - val_mae: 3358.8591\n",
      "Epoch 56/350\n",
      "91/91 - 0s - loss: 10595959.8022 - mae: 2588.7852 - val_loss: 12674765.0000 - val_mae: 2651.4929\n",
      "Epoch 57/350\n",
      "91/91 - 0s - loss: 9955152.4286 - mae: 2608.1792 - val_loss: 11138220.0000 - val_mae: 2827.6218\n",
      "Epoch 58/350\n",
      "91/91 - 0s - loss: 10929366.1978 - mae: 2768.3911 - val_loss: 16560377.0000 - val_mae: 3635.4910\n",
      "Epoch 59/350\n",
      "91/91 - 0s - loss: 12052430.5495 - mae: 2917.9644 - val_loss: 12718909.0000 - val_mae: 2890.3269\n",
      "Epoch 60/350\n",
      "91/91 - 0s - loss: 11726202.0769 - mae: 2779.9766 - val_loss: 17114446.0000 - val_mae: 3683.2073\n",
      "Epoch 61/350\n",
      "91/91 - 0s - loss: 10445934.9121 - mae: 2689.6182 - val_loss: 14000368.0000 - val_mae: 3013.4941\n",
      "Epoch 62/350\n",
      "91/91 - 0s - loss: 10796155.8022 - mae: 2685.4688 - val_loss: 15623707.0000 - val_mae: 3421.5974\n",
      "Epoch 63/350\n",
      "91/91 - 0s - loss: 9829728.9670 - mae: 2524.0374 - val_loss: 13184791.0000 - val_mae: 3189.1887\n",
      "Epoch 64/350\n",
      "91/91 - 0s - loss: 10325703.1538 - mae: 2592.2314 - val_loss: 15995025.0000 - val_mae: 2837.0625\n",
      "Epoch 65/350\n",
      "91/91 - 0s - loss: 9051165.6264 - mae: 2455.1831 - val_loss: 13309581.0000 - val_mae: 2571.5847\n",
      "Epoch 66/350\n",
      "91/91 - 0s - loss: 9241550.2527 - mae: 2503.4131 - val_loss: 13504893.0000 - val_mae: 2847.3525\n",
      "Epoch 67/350\n",
      "91/91 - 0s - loss: 10172223.8407 - mae: 2542.8538 - val_loss: 15997480.0000 - val_mae: 3330.7236\n",
      "Epoch 68/350\n",
      "91/91 - 0s - loss: 9371280.1648 - mae: 2400.6589 - val_loss: 12992448.0000 - val_mae: 2759.4290\n",
      "Epoch 69/350\n",
      "91/91 - 0s - loss: 9164337.7363 - mae: 2352.9573 - val_loss: 21801718.0000 - val_mae: 3807.7656\n",
      "Epoch 70/350\n",
      "91/91 - 0s - loss: 8590682.3791 - mae: 2270.7012 - val_loss: 22784842.0000 - val_mae: 3885.3105\n",
      "Epoch 71/350\n",
      "91/91 - 0s - loss: 7573668.0110 - mae: 2163.1619 - val_loss: 19933150.0000 - val_mae: 3595.9617\n",
      "Epoch 72/350\n",
      "91/91 - 0s - loss: 7514965.7912 - mae: 2090.6335 - val_loss: 20314552.0000 - val_mae: 3630.8572\n",
      "Epoch 73/350\n",
      "91/91 - 0s - loss: 8119127.5824 - mae: 2168.0657 - val_loss: 22265896.0000 - val_mae: 3687.4602\n",
      "Epoch 74/350\n",
      "91/91 - 0s - loss: 9003843.6154 - mae: 2339.9006 - val_loss: 24443114.0000 - val_mae: 4118.1064\n",
      "Epoch 75/350\n",
      "91/91 - 0s - loss: 9748765.9780 - mae: 2462.8350 - val_loss: 25587130.0000 - val_mae: 4086.8777\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 76/350\n",
      "91/91 - 0s - loss: 9091034.8297 - mae: 2392.7390 - val_loss: 20894736.0000 - val_mae: 3558.3992\n",
      "Epoch 77/350\n",
      "91/91 - 0s - loss: 9359235.3846 - mae: 2381.2129 - val_loss: 21347470.0000 - val_mae: 3599.7559\n",
      "Epoch 78/350\n",
      "91/91 - 0s - loss: 9328261.5604 - mae: 2461.4390 - val_loss: 21419842.0000 - val_mae: 3637.3987\n",
      "Epoch 79/350\n",
      "91/91 - 0s - loss: 9328150.9780 - mae: 2469.2249 - val_loss: 19046442.0000 - val_mae: 3647.8213\n",
      "Epoch 80/350\n",
      "91/91 - 0s - loss: 10013980.4396 - mae: 2515.2556 - val_loss: 15983536.0000 - val_mae: 3284.9746\n",
      "Epoch 81/350\n",
      "91/91 - 0s - loss: 9622274.4945 - mae: 2432.0625 - val_loss: 12775969.0000 - val_mae: 2969.9773\n",
      "Epoch 82/350\n",
      "91/91 - 0s - loss: 9692446.6978 - mae: 2413.2576 - val_loss: 11334764.0000 - val_mae: 2961.6340\n",
      "Epoch 83/350\n",
      "91/91 - 0s - loss: 8852744.4945 - mae: 2358.8850 - val_loss: 11501339.0000 - val_mae: 3102.7512\n",
      "Epoch 84/350\n",
      "91/91 - 0s - loss: 8825069.3187 - mae: 2336.6016 - val_loss: 8600237.0000 - val_mae: 2570.2249\n",
      "Epoch 85/350\n",
      "91/91 - 0s - loss: 8589909.4231 - mae: 2351.5222 - val_loss: 7608604.0000 - val_mae: 2391.2595\n",
      "Epoch 86/350\n",
      "91/91 - 0s - loss: 8490987.3626 - mae: 2360.9128 - val_loss: 8293280.5000 - val_mae: 2524.2776\n",
      "Epoch 87/350\n",
      "91/91 - 0s - loss: 7677878.8791 - mae: 2208.4854 - val_loss: 10079387.0000 - val_mae: 2240.6135\n",
      "Epoch 88/350\n",
      "91/91 - 0s - loss: 9913244.8901 - mae: 2534.6257 - val_loss: 26198112.0000 - val_mae: 4334.5181\n",
      "Epoch 89/350\n",
      "91/91 - 0s - loss: 12292768.4286 - mae: 2682.5469 - val_loss: 26930114.0000 - val_mae: 4332.7407\n",
      "Epoch 90/350\n",
      "91/91 - 0s - loss: 21241012.0220 - mae: 3614.8503 - val_loss: 19857006.0000 - val_mae: 3684.9551\n",
      "Epoch 91/350\n",
      "91/91 - 0s - loss: 16717024.7033 - mae: 3274.2112 - val_loss: 14010021.0000 - val_mae: 3106.4111\n",
      "Epoch 92/350\n",
      "91/91 - 0s - loss: 17254353.0989 - mae: 3226.9392 - val_loss: 13526389.0000 - val_mae: 3245.7878\n",
      "Epoch 93/350\n",
      "91/91 - 0s - loss: 18315324.5934 - mae: 3226.7500 - val_loss: 17267492.0000 - val_mae: 3897.4270\n",
      "Epoch 94/350\n",
      "91/91 - 0s - loss: 18652007.6703 - mae: 3350.2534 - val_loss: 14584717.0000 - val_mae: 2947.4231\n",
      "Epoch 95/350\n",
      "91/91 - 0s - loss: 17672266.4176 - mae: 3249.3887 - val_loss: 14378328.0000 - val_mae: 3042.1907\n",
      "Epoch 96/350\n",
      "91/91 - 0s - loss: 14518374.2198 - mae: 3076.3455 - val_loss: 15667737.0000 - val_mae: 3655.8088\n",
      "Epoch 97/350\n",
      "91/91 - 0s - loss: 13738910.9945 - mae: 3029.5779 - val_loss: 16664609.0000 - val_mae: 3119.0830\n",
      "Epoch 98/350\n",
      "91/91 - 0s - loss: 13076456.1319 - mae: 2846.7458 - val_loss: 15446508.0000 - val_mae: 3031.6514\n",
      "Epoch 99/350\n",
      "91/91 - 0s - loss: 12203762.8901 - mae: 2781.9729 - val_loss: 13183805.0000 - val_mae: 3140.4617\n",
      "Epoch 100/350\n",
      "91/91 - 0s - loss: 10560678.4505 - mae: 2612.8140 - val_loss: 14554056.0000 - val_mae: 2934.9968\n",
      "Epoch 101/350\n",
      "91/91 - 0s - loss: 11020882.9835 - mae: 2577.9089 - val_loss: 13240263.0000 - val_mae: 3161.3977\n",
      "Epoch 102/350\n",
      "91/91 - 0s - loss: 11311436.5165 - mae: 2727.2112 - val_loss: 15931328.0000 - val_mae: 3634.1335\n",
      "Epoch 103/350\n",
      "91/91 - 0s - loss: 11096945.8791 - mae: 2668.2219 - val_loss: 17310026.0000 - val_mae: 3004.2256\n",
      "Epoch 104/350\n",
      "91/91 - 0s - loss: 9929828.1648 - mae: 2569.1614 - val_loss: 15359519.0000 - val_mae: 3416.4033\n",
      "Epoch 105/350\n",
      "91/91 - 0s - loss: 10975617.8681 - mae: 2733.2893 - val_loss: 10849987.0000 - val_mae: 2843.6550\n",
      "Epoch 106/350\n",
      "91/91 - 0s - loss: 11858113.0769 - mae: 2822.1384 - val_loss: 12279972.0000 - val_mae: 2609.6509\n",
      "Epoch 107/350\n",
      "91/91 - 0s - loss: 10756431.1209 - mae: 2621.4351 - val_loss: 18585880.0000 - val_mae: 3831.8425\n",
      "Epoch 108/350\n",
      "91/91 - 0s - loss: 12233149.5824 - mae: 2752.1599 - val_loss: 18425548.0000 - val_mae: 3874.3652\n",
      "Epoch 109/350\n",
      "91/91 - 0s - loss: 13407795.4286 - mae: 2890.6729 - val_loss: 16928804.0000 - val_mae: 3172.3308\n",
      "Epoch 110/350\n",
      "91/91 - 0s - loss: 12372655.7802 - mae: 2814.2034 - val_loss: 15159669.0000 - val_mae: 3642.4902\n",
      "Epoch 111/350\n",
      "91/91 - 0s - loss: 10426700.0989 - mae: 2575.0461 - val_loss: 15506235.0000 - val_mae: 3170.5300\n",
      "Epoch 112/350\n",
      "91/91 - 0s - loss: 9566479.7473 - mae: 2433.6479 - val_loss: 16741436.0000 - val_mae: 3578.3298\n",
      "Epoch 113/350\n",
      "91/91 - 0s - loss: 8595488.3242 - mae: 2337.5813 - val_loss: 15591064.0000 - val_mae: 3520.1238\n",
      "Epoch 114/350\n",
      "91/91 - 0s - loss: 7606956.3297 - mae: 2209.2773 - val_loss: 14679896.0000 - val_mae: 3351.0088\n",
      "Epoch 115/350\n",
      "91/91 - 0s - loss: 7221752.4615 - mae: 2096.2334 - val_loss: 13790689.0000 - val_mae: 3037.6963\n",
      "Epoch 116/350\n",
      "91/91 - 0s - loss: 7440479.5989 - mae: 2176.6624 - val_loss: 13072993.0000 - val_mae: 2965.3777\n",
      "Epoch 117/350\n",
      "91/91 - 0s - loss: 7284103.4505 - mae: 2070.7417 - val_loss: 12669791.0000 - val_mae: 2744.0891\n",
      "Epoch 118/350\n",
      "91/91 - 0s - loss: 7057851.7692 - mae: 1996.6334 - val_loss: 14629043.0000 - val_mae: 3371.7410\n",
      "Epoch 119/350\n",
      "91/91 - 0s - loss: 7312331.0330 - mae: 2052.7424 - val_loss: 13055451.0000 - val_mae: 2949.4111\n",
      "Epoch 120/350\n",
      "91/91 - 0s - loss: 7596549.2088 - mae: 2059.7622 - val_loss: 12698384.0000 - val_mae: 3056.9861\n",
      "Epoch 121/350\n",
      "91/91 - 0s - loss: 8246405.8626 - mae: 2291.3752 - val_loss: 13120291.0000 - val_mae: 3123.0439\n",
      "Epoch 122/350\n",
      "91/91 - 0s - loss: 7603867.6429 - mae: 2154.8330 - val_loss: 12263168.0000 - val_mae: 2779.3220\n",
      "Epoch 123/350\n",
      "91/91 - 0s - loss: 8269246.2198 - mae: 2149.0522 - val_loss: 12971243.0000 - val_mae: 3088.1140\n",
      "Epoch 124/350\n",
      "91/91 - 0s - loss: 8715924.7527 - mae: 2338.2039 - val_loss: 13256520.0000 - val_mae: 3127.6238\n",
      "Epoch 125/350\n",
      "91/91 - 0s - loss: 7578084.0385 - mae: 2094.9119 - val_loss: 12953304.0000 - val_mae: 2791.8933\n",
      "Epoch 126/350\n",
      "91/91 - 0s - loss: 7804707.0220 - mae: 2090.6606 - val_loss: 12465847.0000 - val_mae: 2991.5793\n",
      "Epoch 127/350\n",
      "91/91 - 0s - loss: 8145522.7637 - mae: 2290.6729 - val_loss: 12098376.0000 - val_mae: 2929.4521\n",
      "Epoch 128/350\n",
      "91/91 - 0s - loss: 7220652.5687 - mae: 2053.8401 - val_loss: 12560381.0000 - val_mae: 2701.7263\n",
      "Epoch 129/350\n",
      "91/91 - 0s - loss: 8068132.7088 - mae: 2081.8301 - val_loss: 11443751.0000 - val_mae: 2814.3379\n",
      "Epoch 130/350\n",
      "91/91 - 0s - loss: 7168369.1154 - mae: 2067.7026 - val_loss: 12975561.0000 - val_mae: 3145.6692\n",
      "Epoch 131/350\n",
      "91/91 - 0s - loss: 7442754.5604 - mae: 2141.5769 - val_loss: 11825663.0000 - val_mae: 2796.9827\n",
      "Epoch 132/350\n",
      "91/91 - 0s - loss: 7389583.3352 - mae: 1985.0199 - val_loss: 11781560.0000 - val_mae: 2853.0681\n",
      "Epoch 133/350\n",
      "91/91 - 0s - loss: 7951335.1648 - mae: 2224.1345 - val_loss: 12415173.0000 - val_mae: 3011.2131\n",
      "Epoch 134/350\n",
      "91/91 - 0s - loss: 7372890.8462 - mae: 2008.8716 - val_loss: 12053213.0000 - val_mae: 2608.3108\n",
      "Epoch 135/350\n",
      "91/91 - 0s - loss: 7441929.5220 - mae: 2072.4749 - val_loss: 12490759.0000 - val_mae: 3080.5703\n",
      "Epoch 136/350\n",
      "91/91 - 0s - loss: 7296638.9011 - mae: 2147.3789 - val_loss: 11217377.0000 - val_mae: 2698.8684\n",
      "Epoch 137/350\n",
      "91/91 - 0s - loss: 7005803.5330 - mae: 1958.7306 - val_loss: 11335469.0000 - val_mae: 2795.8711\n",
      "Epoch 138/350\n",
      "91/91 - 0s - loss: 6975910.1538 - mae: 1974.4971 - val_loss: 11993299.0000 - val_mae: 2973.3835\n",
      "Epoch 139/350\n",
      "91/91 - 0s - loss: 7567283.5055 - mae: 2198.2429 - val_loss: 11763872.0000 - val_mae: 2878.0627\n",
      "Epoch 140/350\n",
      "91/91 - 0s - loss: 6825922.8297 - mae: 1956.3401 - val_loss: 11683339.0000 - val_mae: 2615.2532\n",
      "Epoch 141/350\n",
      "91/91 - 0s - loss: 7227959.8571 - mae: 2066.0037 - val_loss: 12215283.0000 - val_mae: 3025.6907\n",
      "Epoch 142/350\n",
      "91/91 - 0s - loss: 6875921.7692 - mae: 2042.2273 - val_loss: 11678743.0000 - val_mae: 2732.4050\n",
      "Epoch 143/350\n",
      "91/91 - 0s - loss: 6964795.8874 - mae: 2037.9669 - val_loss: 11802312.0000 - val_mae: 2809.6660\n",
      "Epoch 144/350\n",
      "91/91 - 0s - loss: 6977867.6813 - mae: 2055.7417 - val_loss: 11612219.0000 - val_mae: 2722.3943\n",
      "Epoch 145/350\n",
      "91/91 - 0s - loss: 7107290.7143 - mae: 2007.3918 - val_loss: 12023699.0000 - val_mae: 2829.6160\n",
      "Epoch 146/350\n",
      "91/91 - 0s - loss: 7280534.9451 - mae: 2078.1416 - val_loss: 12691027.0000 - val_mae: 2968.9473\n",
      "Epoch 147/350\n",
      "91/91 - 0s - loss: 7304362.0714 - mae: 2115.9844 - val_loss: 12393597.0000 - val_mae: 2919.7551\n",
      "Epoch 148/350\n",
      "91/91 - 0s - loss: 7312247.8407 - mae: 2032.9148 - val_loss: 12551336.0000 - val_mae: 2906.1174\n",
      "Epoch 149/350\n",
      "91/91 - 0s - loss: 7461443.3736 - mae: 2120.4365 - val_loss: 12578357.0000 - val_mae: 2863.9514\n",
      "Epoch 150/350\n",
      "91/91 - 0s - loss: 7337019.3022 - mae: 2041.5060 - val_loss: 12507603.0000 - val_mae: 2856.0872\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 151/350\n",
      "91/91 - 0s - loss: 7158863.7253 - mae: 2053.8335 - val_loss: 13244420.0000 - val_mae: 2935.1602\n",
      "Epoch 152/350\n",
      "91/91 - 0s - loss: 7036683.9560 - mae: 2118.9441 - val_loss: 12596555.0000 - val_mae: 2875.2810\n",
      "Epoch 153/350\n",
      "91/91 - 0s - loss: 7279112.9396 - mae: 2005.2812 - val_loss: 12722548.0000 - val_mae: 2915.6768\n",
      "Epoch 154/350\n",
      "91/91 - 0s - loss: 7018630.2280 - mae: 2079.6379 - val_loss: 12956220.0000 - val_mae: 2929.5896\n",
      "Epoch 155/350\n",
      "91/91 - 0s - loss: 6872180.8077 - mae: 2004.4935 - val_loss: 12902683.0000 - val_mae: 2914.4900\n",
      "Epoch 156/350\n",
      "91/91 - 0s - loss: 6972174.8846 - mae: 1945.7103 - val_loss: 13341579.0000 - val_mae: 2939.7424\n",
      "Epoch 157/350\n",
      "91/91 - 0s - loss: 6811951.4231 - mae: 1996.8563 - val_loss: 13643339.0000 - val_mae: 2957.4636\n",
      "Epoch 158/350\n",
      "91/91 - 0s - loss: 6845220.1374 - mae: 2007.4611 - val_loss: 13448268.0000 - val_mae: 2952.5896\n",
      "Epoch 159/350\n",
      "91/91 - 0s - loss: 6886416.7418 - mae: 1941.2098 - val_loss: 13565673.0000 - val_mae: 2981.9258\n",
      "Epoch 160/350\n",
      "91/91 - 0s - loss: 6722522.5824 - mae: 1974.0514 - val_loss: 14271233.0000 - val_mae: 3034.8054\n",
      "Epoch 161/350\n",
      "91/91 - 0s - loss: 6879073.3407 - mae: 2042.1152 - val_loss: 13375544.0000 - val_mae: 2978.2761\n",
      "Epoch 162/350\n",
      "91/91 - 0s - loss: 6777494.3626 - mae: 1930.4689 - val_loss: 13532557.0000 - val_mae: 3001.0715\n",
      "Epoch 163/350\n",
      "91/91 - 0s - loss: 6766730.7473 - mae: 1979.0007 - val_loss: 13642160.0000 - val_mae: 3006.9363\n",
      "Epoch 164/350\n",
      "91/91 - 0s - loss: 6804857.4725 - mae: 1993.9357 - val_loss: 13639933.0000 - val_mae: 3013.0295\n",
      "Epoch 165/350\n",
      "91/91 - 0s - loss: 6739809.8187 - mae: 1943.7609 - val_loss: 13460915.0000 - val_mae: 2996.0876\n",
      "Epoch 166/350\n",
      "91/91 - 0s - loss: 6737627.9176 - mae: 1924.4932 - val_loss: 14003416.0000 - val_mae: 3036.2656\n",
      "Epoch 167/350\n",
      "91/91 - 0s - loss: 6717909.6044 - mae: 2015.5287 - val_loss: 13881672.0000 - val_mae: 3035.4893\n",
      "Epoch 168/350\n",
      "91/91 - 0s - loss: 6713950.8901 - mae: 1955.3915 - val_loss: 13407512.0000 - val_mae: 3001.2053\n",
      "Epoch 169/350\n",
      "91/91 - 0s - loss: 6639263.2857 - mae: 1966.5487 - val_loss: 13725080.0000 - val_mae: 3014.1238\n",
      "Epoch 170/350\n",
      "91/91 - 0s - loss: 6679951.6429 - mae: 1989.7061 - val_loss: 13286897.0000 - val_mae: 2976.3684\n",
      "Epoch 171/350\n",
      "91/91 - 0s - loss: 6929854.8901 - mae: 1935.6790 - val_loss: 13552576.0000 - val_mae: 2999.4192\n",
      "Epoch 172/350\n",
      "91/91 - 0s - loss: 6706564.9451 - mae: 2033.2898 - val_loss: 14888760.0000 - val_mae: 3110.0486\n",
      "Epoch 173/350\n",
      "91/91 - 0s - loss: 6813478.9945 - mae: 2045.3627 - val_loss: 13449544.0000 - val_mae: 2963.9348\n",
      "Epoch 174/350\n",
      "91/91 - 0s - loss: 6887498.7857 - mae: 1949.9442 - val_loss: 14045461.0000 - val_mae: 3027.5039\n",
      "Epoch 175/350\n",
      "91/91 - 0s - loss: 6634739.9890 - mae: 1960.7318 - val_loss: 13434939.0000 - val_mae: 2990.1738\n",
      "Epoch 176/350\n",
      "91/91 - 0s - loss: 6877819.0440 - mae: 2034.0818 - val_loss: 13571432.0000 - val_mae: 3000.8982\n",
      "Epoch 177/350\n",
      "91/91 - 0s - loss: 6519221.5220 - mae: 1892.3683 - val_loss: 13362099.0000 - val_mae: 2972.4177\n",
      "Epoch 178/350\n",
      "91/91 - 0s - loss: 6707246.3352 - mae: 1964.5604 - val_loss: 14164948.0000 - val_mae: 3041.3904\n",
      "Epoch 179/350\n",
      "91/91 - 0s - loss: 6557831.8516 - mae: 1992.4136 - val_loss: 13409923.0000 - val_mae: 2987.7175\n",
      "Epoch 180/350\n",
      "91/91 - 0s - loss: 6968013.9780 - mae: 1943.7429 - val_loss: 13793744.0000 - val_mae: 3007.0117\n",
      "Epoch 181/350\n",
      "91/91 - 0s - loss: 6805466.0769 - mae: 2050.2939 - val_loss: 14696048.0000 - val_mae: 3059.2607\n",
      "Epoch 182/350\n",
      "91/91 - 0s - loss: 6571318.6978 - mae: 1949.0916 - val_loss: 13395629.0000 - val_mae: 2971.1096\n",
      "Epoch 183/350\n",
      "91/91 - 0s - loss: 6682386.9286 - mae: 1923.5364 - val_loss: 13895659.0000 - val_mae: 3030.3992\n",
      "Epoch 184/350\n",
      "91/91 - 0s - loss: 7116272.8681 - mae: 2105.5884 - val_loss: 13585357.0000 - val_mae: 3007.6443\n",
      "Epoch 185/350\n",
      "91/91 - 0s - loss: 7411396.4396 - mae: 2030.8652 - val_loss: 13608903.0000 - val_mae: 2998.1833\n",
      "Epoch 186/350\n",
      "91/91 - 0s - loss: 8341253.5495 - mae: 2284.8889 - val_loss: 15602557.0000 - val_mae: 3191.5583\n",
      "Epoch 187/350\n",
      "91/91 - 0s - loss: 6951408.7473 - mae: 1981.6963 - val_loss: 14240341.0000 - val_mae: 3008.3704\n",
      "Epoch 188/350\n",
      "91/91 - 0s - loss: 7151453.3132 - mae: 1981.4674 - val_loss: 16130109.0000 - val_mae: 3313.1829\n",
      "Epoch 189/350\n",
      "91/91 - 0s - loss: 7049637.6538 - mae: 2154.3706 - val_loss: 13430212.0000 - val_mae: 2991.9724\n",
      "Epoch 190/350\n",
      "91/91 - 0s - loss: 6693208.7473 - mae: 1919.9172 - val_loss: 13314261.0000 - val_mae: 2956.0925\n",
      "Epoch 191/350\n",
      "91/91 - 0s - loss: 6491844.9066 - mae: 1915.6213 - val_loss: 15409191.0000 - val_mae: 3150.0916\n",
      "Epoch 192/350\n",
      "91/91 - 0s - loss: 6694877.1209 - mae: 2059.4065 - val_loss: 13336525.0000 - val_mae: 2962.4424\n",
      "Epoch 193/350\n",
      "91/91 - 0s - loss: 6627576.7363 - mae: 1912.2957 - val_loss: 13488775.0000 - val_mae: 2988.3098\n",
      "Epoch 194/350\n",
      "91/91 - 0s - loss: 6702644.1703 - mae: 1982.1432 - val_loss: 13696819.0000 - val_mae: 3004.4382\n",
      "Epoch 195/350\n",
      "91/91 - 0s - loss: 6573651.3077 - mae: 1891.0992 - val_loss: 13768229.0000 - val_mae: 3010.6638\n",
      "Epoch 196/350\n",
      "91/91 - 0s - loss: 6335544.5495 - mae: 1888.2246 - val_loss: 14728875.0000 - val_mae: 3102.9407\n",
      "Epoch 197/350\n",
      "91/91 - 0s - loss: 6730960.9396 - mae: 2088.5071 - val_loss: 13840656.0000 - val_mae: 3023.1475\n",
      "Epoch 198/350\n",
      "91/91 - 0s - loss: 7186338.0659 - mae: 1997.5232 - val_loss: 13459859.0000 - val_mae: 2988.3008\n",
      "Epoch 199/350\n",
      "91/91 - 0s - loss: 7388517.4231 - mae: 2187.8643 - val_loss: 15652456.0000 - val_mae: 3225.8184\n",
      "Epoch 200/350\n",
      "91/91 - 0s - loss: 7717969.3626 - mae: 2101.5583 - val_loss: 13677473.0000 - val_mae: 2963.1257\n",
      "Epoch 201/350\n",
      "91/91 - 0s - loss: 6373977.4176 - mae: 1936.5261 - val_loss: 17250918.0000 - val_mae: 3413.4336\n",
      "Epoch 202/350\n",
      "91/91 - 0s - loss: 7341765.4176 - mae: 2229.8333 - val_loss: 13629013.0000 - val_mae: 2961.8625\n",
      "Epoch 203/350\n",
      "91/91 - 0s - loss: 7013081.8077 - mae: 1973.0723 - val_loss: 13366609.0000 - val_mae: 2957.8879\n",
      "Epoch 204/350\n",
      "91/91 - 0s - loss: 6347929.4945 - mae: 1911.3989 - val_loss: 15526683.0000 - val_mae: 3215.2273\n",
      "Epoch 205/350\n",
      "91/91 - 0s - loss: 6935862.2088 - mae: 2146.9841 - val_loss: 13559843.0000 - val_mae: 2986.6008\n",
      "Epoch 206/350\n",
      "91/91 - 0s - loss: 6501378.7692 - mae: 1911.3007 - val_loss: 13620555.0000 - val_mae: 2973.7571\n",
      "Epoch 207/350\n",
      "91/91 - 0s - loss: 6589246.9121 - mae: 1944.7729 - val_loss: 15064419.0000 - val_mae: 3105.8103\n",
      "Epoch 208/350\n",
      "91/91 - 0s - loss: 7071157.3516 - mae: 2043.2947 - val_loss: 13969541.0000 - val_mae: 3009.2559\n",
      "Epoch 209/350\n",
      "91/91 - 0s - loss: 6518622.1099 - mae: 1960.7140 - val_loss: 14800933.0000 - val_mae: 3106.4680\n",
      "Epoch 210/350\n",
      "91/91 - 0s - loss: 6575876.2143 - mae: 2005.6555 - val_loss: 13717455.0000 - val_mae: 2981.2393\n",
      "Epoch 211/350\n",
      "91/91 - 0s - loss: 6615764.9451 - mae: 1919.7242 - val_loss: 14333837.0000 - val_mae: 3023.8533\n",
      "Epoch 212/350\n",
      "91/91 - 0s - loss: 6614827.1813 - mae: 1979.5968 - val_loss: 13850631.0000 - val_mae: 2981.1223\n",
      "Epoch 213/350\n",
      "91/91 - 0s - loss: 6443555.0385 - mae: 1940.5366 - val_loss: 13803795.0000 - val_mae: 2968.5000\n",
      "Epoch 214/350\n",
      "91/91 - 0s - loss: 6396865.4780 - mae: 1925.6964 - val_loss: 13699085.0000 - val_mae: 2958.6160\n",
      "Epoch 215/350\n",
      "91/91 - 0s - loss: 6516160.2582 - mae: 1936.2094 - val_loss: 14311571.0000 - val_mae: 2991.7341\n",
      "Epoch 216/350\n",
      "91/91 - 0s - loss: 6414523.8571 - mae: 1973.3403 - val_loss: 13611296.0000 - val_mae: 2970.6650\n",
      "Epoch 217/350\n",
      "91/91 - 0s - loss: 6339044.8352 - mae: 1916.4194 - val_loss: 13716672.0000 - val_mae: 2981.9365\n",
      "Epoch 218/350\n",
      "91/91 - 0s - loss: 6389446.1923 - mae: 1951.5681 - val_loss: 14003872.0000 - val_mae: 2996.7004\n",
      "Epoch 219/350\n",
      "91/91 - 0s - loss: 6445010.4560 - mae: 1984.3749 - val_loss: 13751931.0000 - val_mae: 2973.1921\n",
      "Epoch 220/350\n",
      "91/91 - 0s - loss: 6518169.6648 - mae: 1911.5438 - val_loss: 13747217.0000 - val_mae: 2987.4583\n",
      "Epoch 221/350\n",
      "91/91 - 0s - loss: 6508576.9835 - mae: 2006.7935 - val_loss: 14605668.0000 - val_mae: 3037.9055\n",
      "Epoch 222/350\n",
      "91/91 - 0s - loss: 6248104.4615 - mae: 1924.8641 - val_loss: 13616864.0000 - val_mae: 2966.8699\n",
      "Epoch 223/350\n",
      "91/91 - 0s - loss: 6402984.1978 - mae: 1902.5424 - val_loss: 13895800.0000 - val_mae: 2990.5540\n",
      "Epoch 224/350\n",
      "91/91 - 0s - loss: 6235614.2857 - mae: 1908.0807 - val_loss: 14040643.0000 - val_mae: 2993.9912\n",
      "Epoch 225/350\n",
      "91/91 - 0s - loss: 6287828.5824 - mae: 1934.8420 - val_loss: 13686899.0000 - val_mae: 2974.1941\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 226/350\n",
      "91/91 - 0s - loss: 6246093.9341 - mae: 1899.3472 - val_loss: 13662109.0000 - val_mae: 2978.6238\n",
      "Epoch 227/350\n",
      "91/91 - 0s - loss: 6731703.5220 - mae: 2002.8949 - val_loss: 14348080.0000 - val_mae: 3010.3538\n",
      "Epoch 228/350\n",
      "91/91 - 0s - loss: 7321009.8022 - mae: 2013.9113 - val_loss: 13587171.0000 - val_mae: 2966.2942\n",
      "Epoch 229/350\n",
      "91/91 - 0s - loss: 6029492.0440 - mae: 1918.2178 - val_loss: 16570471.0000 - val_mae: 3310.2048\n",
      "Epoch 230/350\n",
      "91/91 - 0s - loss: 6734833.9451 - mae: 2087.7241 - val_loss: 13556020.0000 - val_mae: 2945.7881\n",
      "Epoch 231/350\n",
      "91/91 - 0s - loss: 6662132.2033 - mae: 1921.4886 - val_loss: 13789525.0000 - val_mae: 3002.0793\n",
      "Epoch 232/350\n",
      "91/91 - 0s - loss: 6426888.7308 - mae: 2001.5505 - val_loss: 15229681.0000 - val_mae: 3144.3191\n",
      "Epoch 233/350\n",
      "91/91 - 0s - loss: 6370880.9890 - mae: 1968.1017 - val_loss: 13520349.0000 - val_mae: 2935.6348\n",
      "Epoch 234/350\n",
      "91/91 - 0s - loss: 6540635.7418 - mae: 1911.8422 - val_loss: 15092637.0000 - val_mae: 3097.6445\n",
      "Epoch 235/350\n",
      "91/91 - 0s - loss: 6319445.2582 - mae: 2003.7332 - val_loss: 13627616.0000 - val_mae: 2963.8552\n",
      "Epoch 236/350\n",
      "91/91 - 0s - loss: 6577456.5604 - mae: 1893.2479 - val_loss: 13676600.0000 - val_mae: 2980.6995\n",
      "Epoch 237/350\n",
      "91/91 - 0s - loss: 6198466.6044 - mae: 1961.2657 - val_loss: 15627797.0000 - val_mae: 3181.9929\n",
      "Epoch 238/350\n",
      "91/91 - 0s - loss: 6399946.6758 - mae: 2019.5457 - val_loss: 13527104.0000 - val_mae: 2980.5020\n",
      "Epoch 239/350\n",
      "91/91 - 0s - loss: 6238163.0412 - mae: 1857.4772 - val_loss: 14521872.0000 - val_mae: 3060.8350\n",
      "Epoch 240/350\n",
      "91/91 - 0s - loss: 6225418.6813 - mae: 1949.4530 - val_loss: 14710693.0000 - val_mae: 3074.4666\n",
      "Epoch 241/350\n",
      "91/91 - 0s - loss: 6163092.9341 - mae: 1958.9358 - val_loss: 13640396.0000 - val_mae: 2976.0403\n",
      "Epoch 242/350\n",
      "91/91 - 0s - loss: 6491296.2198 - mae: 1888.5983 - val_loss: 13863465.0000 - val_mae: 3013.6306\n",
      "Epoch 243/350\n",
      "91/91 - 0s - loss: 6339164.9505 - mae: 1977.8167 - val_loss: 15037808.0000 - val_mae: 3105.2012\n",
      "Epoch 244/350\n",
      "91/91 - 0s - loss: 6452735.6209 - mae: 1944.9540 - val_loss: 13560596.0000 - val_mae: 2943.9504\n",
      "Epoch 245/350\n",
      "91/91 - 0s - loss: 6169565.7527 - mae: 1903.4076 - val_loss: 14833667.0000 - val_mae: 3073.7014\n",
      "Epoch 246/350\n",
      "91/91 - 0s - loss: 6141860.3791 - mae: 1933.7758 - val_loss: 13528256.0000 - val_mae: 2968.8484\n",
      "Epoch 247/350\n",
      "91/91 - 0s - loss: 6349119.2088 - mae: 1849.2970 - val_loss: 14101628.0000 - val_mae: 3017.2937\n",
      "Epoch 248/350\n",
      "91/91 - 0s - loss: 7203901.0110 - mae: 2193.5557 - val_loss: 13951647.0000 - val_mae: 3004.8018\n",
      "Epoch 249/350\n",
      "91/91 - 0s - loss: 6635691.8571 - mae: 1929.0514 - val_loss: 13193297.0000 - val_mae: 2916.7117\n",
      "Epoch 250/350\n",
      "91/91 - 0s - loss: 6235396.0604 - mae: 1925.7351 - val_loss: 16973896.0000 - val_mae: 3442.1533\n",
      "Epoch 251/350\n",
      "91/91 - 0s - loss: 7087339.5549 - mae: 2125.2000 - val_loss: 13084136.0000 - val_mae: 2910.5144\n",
      "Epoch 252/350\n",
      "91/91 - 0s - loss: 6119530.4451 - mae: 1828.4220 - val_loss: 14544635.0000 - val_mae: 3071.4355\n",
      "Epoch 253/350\n",
      "91/91 - 0s - loss: 6199035.7253 - mae: 1989.8510 - val_loss: 13914923.0000 - val_mae: 2990.6536\n",
      "Epoch 254/350\n",
      "91/91 - 0s - loss: 6114952.8352 - mae: 1889.3657 - val_loss: 13425048.0000 - val_mae: 2934.2219\n",
      "Epoch 255/350\n",
      "91/91 - 0s - loss: 6564339.1429 - mae: 2003.6367 - val_loss: 14766843.0000 - val_mae: 3057.8035\n",
      "Epoch 256/350\n",
      "91/91 - 0s - loss: 6018373.9835 - mae: 1865.4169 - val_loss: 13486059.0000 - val_mae: 2953.5525\n",
      "Epoch 257/350\n",
      "91/91 - 0s - loss: 6966765.7363 - mae: 2056.7805 - val_loss: 14107232.0000 - val_mae: 3028.6523\n",
      "Epoch 258/350\n",
      "91/91 - 0s - loss: 5878944.7143 - mae: 1854.6099 - val_loss: 13384413.0000 - val_mae: 2962.1204\n",
      "Epoch 259/350\n",
      "91/91 - 0s - loss: 6541013.2308 - mae: 1874.1243 - val_loss: 15142104.0000 - val_mae: 3199.5115\n",
      "Epoch 260/350\n",
      "91/91 - 0s - loss: 7496740.2747 - mae: 2214.3374 - val_loss: 14986488.0000 - val_mae: 3154.5325\n",
      "Epoch 261/350\n",
      "91/91 - 0s - loss: 5883268.2143 - mae: 1819.5553 - val_loss: 14255756.0000 - val_mae: 3106.8069\n",
      "Epoch 262/350\n",
      "91/91 - 0s - loss: 6525571.9258 - mae: 1901.1892 - val_loss: 17284348.0000 - val_mae: 3418.2031\n",
      "Epoch 263/350\n",
      "91/91 - 0s - loss: 6511740.8462 - mae: 2067.8577 - val_loss: 13564835.0000 - val_mae: 2986.0120\n",
      "Epoch 264/350\n",
      "91/91 - 0s - loss: 6676852.0769 - mae: 1949.9059 - val_loss: 13491109.0000 - val_mae: 2981.3066\n",
      "Epoch 265/350\n",
      "91/91 - 0s - loss: 7351224.0659 - mae: 2132.0125 - val_loss: 14720160.0000 - val_mae: 3112.5508\n",
      "Epoch 266/350\n",
      "91/91 - 0s - loss: 6645936.0165 - mae: 1932.7102 - val_loss: 13423767.0000 - val_mae: 2964.0959\n",
      "Epoch 267/350\n",
      "91/91 - 0s - loss: 5662640.2033 - mae: 1768.5343 - val_loss: 17195584.0000 - val_mae: 3350.5105\n",
      "Epoch 268/350\n",
      "91/91 - 0s - loss: 6864273.7637 - mae: 2075.0859 - val_loss: 13495613.0000 - val_mae: 2963.7998\n",
      "Epoch 269/350\n",
      "91/91 - 0s - loss: 6021478.7802 - mae: 1851.2740 - val_loss: 13624528.0000 - val_mae: 2984.4343\n",
      "Epoch 270/350\n",
      "91/91 - 0s - loss: 6278117.6319 - mae: 1955.7476 - val_loss: 14006040.0000 - val_mae: 3013.6350\n",
      "Epoch 271/350\n",
      "91/91 - 0s - loss: 5775039.9286 - mae: 1831.4574 - val_loss: 13332603.0000 - val_mae: 2943.0056\n",
      "Epoch 272/350\n",
      "91/91 - 0s - loss: 6239978.0659 - mae: 1849.7761 - val_loss: 14589245.0000 - val_mae: 3027.4043\n",
      "Epoch 273/350\n",
      "91/91 - 0s - loss: 7135230.9286 - mae: 2162.7583 - val_loss: 14004920.0000 - val_mae: 2995.6257\n",
      "Epoch 274/350\n",
      "91/91 - 0s - loss: 6251685.6264 - mae: 1883.1847 - val_loss: 13300456.0000 - val_mae: 2979.8342\n",
      "Epoch 275/350\n",
      "91/91 - 0s - loss: 6411921.2802 - mae: 1856.4265 - val_loss: 17731758.0000 - val_mae: 3516.6145\n",
      "Epoch 276/350\n",
      "91/91 - 0s - loss: 6995424.6593 - mae: 2099.9993 - val_loss: 12833865.0000 - val_mae: 2932.8147\n",
      "Epoch 277/350\n",
      "91/91 - 0s - loss: 7017119.3736 - mae: 1982.2990 - val_loss: 14281881.0000 - val_mae: 2991.5378\n",
      "Epoch 278/350\n",
      "91/91 - 0s - loss: 6837126.2253 - mae: 2120.4712 - val_loss: 15572929.0000 - val_mae: 3188.6077\n",
      "Epoch 279/350\n",
      "91/91 - 0s - loss: 6136699.2143 - mae: 1880.2239 - val_loss: 13882027.0000 - val_mae: 3053.5090\n",
      "Epoch 280/350\n",
      "91/91 - 0s - loss: 6827756.1648 - mae: 1975.5330 - val_loss: 16345491.0000 - val_mae: 3327.6250\n",
      "Epoch 281/350\n",
      "91/91 - 0s - loss: 6395362.4011 - mae: 2059.1487 - val_loss: 13330731.0000 - val_mae: 2944.3489\n",
      "Epoch 282/350\n",
      "91/91 - 0s - loss: 6528740.9945 - mae: 1893.3467 - val_loss: 13579945.0000 - val_mae: 2960.7903\n",
      "Epoch 283/350\n",
      "91/91 - 0s - loss: 5884839.0934 - mae: 1891.7518 - val_loss: 15897827.0000 - val_mae: 3198.6465\n",
      "Epoch 284/350\n",
      "91/91 - 0s - loss: 6468844.4725 - mae: 2013.6465 - val_loss: 13540997.0000 - val_mae: 2977.4746\n",
      "Epoch 285/350\n",
      "91/91 - 0s - loss: 5769317.9066 - mae: 1799.0913 - val_loss: 16349392.0000 - val_mae: 3209.8372\n",
      "Epoch 286/350\n",
      "91/91 - 0s - loss: 6529309.9011 - mae: 2062.3972 - val_loss: 13724611.0000 - val_mae: 2960.2551\n",
      "Epoch 287/350\n",
      "91/91 - 0s - loss: 6145954.5879 - mae: 1849.6871 - val_loss: 13397661.0000 - val_mae: 2957.2488\n",
      "Epoch 288/350\n",
      "91/91 - 0s - loss: 6521299.9835 - mae: 1927.9110 - val_loss: 16365523.0000 - val_mae: 3332.1692\n",
      "Epoch 289/350\n",
      "91/91 - 0s - loss: 6531629.0165 - mae: 1999.6786 - val_loss: 13238680.0000 - val_mae: 2938.2581\n",
      "Epoch 290/350\n",
      "91/91 - 0s - loss: 5866718.9615 - mae: 1846.1443 - val_loss: 15468389.0000 - val_mae: 3177.7549\n",
      "Epoch 291/350\n",
      "91/91 - 0s - loss: 5903000.7692 - mae: 1924.1456 - val_loss: 13496816.0000 - val_mae: 2949.3691\n",
      "Epoch 292/350\n",
      "91/91 - 0s - loss: 6100146.5824 - mae: 1851.0168 - val_loss: 13790651.0000 - val_mae: 2957.3997\n",
      "Epoch 293/350\n",
      "91/91 - 0s - loss: 5753620.4670 - mae: 1854.4802 - val_loss: 15278760.0000 - val_mae: 3120.6404\n",
      "Epoch 294/350\n",
      "91/91 - 0s - loss: 6035351.5165 - mae: 1891.1772 - val_loss: 13427117.0000 - val_mae: 2956.0938\n",
      "Epoch 295/350\n",
      "91/91 - 0s - loss: 5715111.8791 - mae: 1784.4050 - val_loss: 13829991.0000 - val_mae: 2986.4558\n",
      "Epoch 296/350\n",
      "91/91 - 0s - loss: 5898345.3846 - mae: 1900.7178 - val_loss: 13835821.0000 - val_mae: 2975.8701\n",
      "Epoch 297/350\n",
      "91/91 - 0s - loss: 5564609.9890 - mae: 1795.8300 - val_loss: 13422619.0000 - val_mae: 2940.7461\n",
      "Epoch 298/350\n",
      "91/91 - 0s - loss: 5735787.4615 - mae: 1809.7693 - val_loss: 14611848.0000 - val_mae: 3006.4778\n",
      "Epoch 299/350\n",
      "91/91 - 0s - loss: 5662690.4176 - mae: 1852.8650 - val_loss: 13528179.0000 - val_mae: 2963.8899\n",
      "Epoch 300/350\n",
      "91/91 - 0s - loss: 5602351.0055 - mae: 1787.3940 - val_loss: 14314072.0000 - val_mae: 2927.3933\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 301/350\n",
      "91/91 - 0s - loss: 5661807.6099 - mae: 1820.2559 - val_loss: 14062013.0000 - val_mae: 2918.5842\n",
      "Epoch 302/350\n",
      "91/91 - 0s - loss: 5685184.8407 - mae: 1816.9697 - val_loss: 13616467.0000 - val_mae: 2872.3906\n",
      "Epoch 303/350\n",
      "91/91 - 0s - loss: 5663552.3077 - mae: 1803.4659 - val_loss: 14229609.0000 - val_mae: 2914.8567\n",
      "Epoch 304/350\n",
      "91/91 - 0s - loss: 5631170.7473 - mae: 1850.2817 - val_loss: 13917272.0000 - val_mae: 2883.0547\n",
      "Epoch 305/350\n",
      "91/91 - 0s - loss: 5500648.9341 - mae: 1786.7023 - val_loss: 13324188.0000 - val_mae: 2861.7390\n",
      "Epoch 306/350\n",
      "91/91 - 0s - loss: 5574747.5714 - mae: 1805.4763 - val_loss: 13849575.0000 - val_mae: 2885.3464\n",
      "Epoch 307/350\n",
      "91/91 - 0s - loss: 5645023.6813 - mae: 1774.8209 - val_loss: 13646740.0000 - val_mae: 2863.4343\n",
      "Epoch 308/350\n",
      "91/91 - 0s - loss: 5461054.7418 - mae: 1780.6587 - val_loss: 14578133.0000 - val_mae: 2972.9192\n",
      "Epoch 309/350\n",
      "91/91 - 0s - loss: 5983645.0385 - mae: 1925.0345 - val_loss: 13120960.0000 - val_mae: 2837.7351\n",
      "Epoch 310/350\n",
      "91/91 - 0s - loss: 5842369.5165 - mae: 1800.6350 - val_loss: 13678755.0000 - val_mae: 2866.2786\n",
      "Epoch 311/350\n",
      "91/91 - 0s - loss: 5748482.0769 - mae: 1868.8711 - val_loss: 14039108.0000 - val_mae: 2884.3792\n",
      "Epoch 312/350\n",
      "91/91 - 0s - loss: 5550227.8242 - mae: 1806.0314 - val_loss: 13154829.0000 - val_mae: 2834.6516\n",
      "Epoch 313/350\n",
      "91/91 - 0s - loss: 5483455.4505 - mae: 1763.5157 - val_loss: 14226481.0000 - val_mae: 2897.3162\n",
      "Epoch 314/350\n",
      "91/91 - 0s - loss: 5787927.7637 - mae: 1886.9689 - val_loss: 13294285.0000 - val_mae: 2821.6809\n",
      "Epoch 315/350\n",
      "91/91 - 0s - loss: 5532280.4615 - mae: 1792.7273 - val_loss: 13078275.0000 - val_mae: 2809.6201\n",
      "Epoch 316/350\n",
      "91/91 - 0s - loss: 5509931.0110 - mae: 1766.6296 - val_loss: 14622569.0000 - val_mae: 3011.5598\n",
      "Epoch 317/350\n",
      "91/91 - 0s - loss: 5735211.1044 - mae: 1904.3694 - val_loss: 13554011.0000 - val_mae: 2855.0996\n",
      "Epoch 318/350\n",
      "91/91 - 0s - loss: 5571992.0962 - mae: 1797.8125 - val_loss: 12917249.0000 - val_mae: 2806.6067\n",
      "Epoch 319/350\n",
      "91/91 - 0s - loss: 5631538.5934 - mae: 1824.1029 - val_loss: 15218359.0000 - val_mae: 3074.7637\n",
      "Epoch 320/350\n",
      "91/91 - 0s - loss: 5627351.1429 - mae: 1799.0811 - val_loss: 12956044.0000 - val_mae: 2812.3037\n",
      "Epoch 321/350\n",
      "91/91 - 0s - loss: 5693502.0989 - mae: 1853.1538 - val_loss: 13519443.0000 - val_mae: 2850.2969\n",
      "Epoch 322/350\n",
      "91/91 - 0s - loss: 5442108.6099 - mae: 1758.2860 - val_loss: 13257395.0000 - val_mae: 2829.7820\n",
      "Epoch 323/350\n",
      "91/91 - 0s - loss: 5406830.6484 - mae: 1754.9569 - val_loss: 14452099.0000 - val_mae: 2944.7842\n",
      "Epoch 324/350\n",
      "91/91 - 0s - loss: 5550439.7253 - mae: 1853.5208 - val_loss: 13234509.0000 - val_mae: 2812.7087\n",
      "Epoch 325/350\n",
      "91/91 - 0s - loss: 5651684.5110 - mae: 1789.5304 - val_loss: 13079541.0000 - val_mae: 2817.2610\n",
      "Epoch 326/350\n",
      "91/91 - 0s - loss: 5680900.6758 - mae: 1830.9324 - val_loss: 14200008.0000 - val_mae: 2941.5002\n",
      "Epoch 327/350\n",
      "91/91 - 0s - loss: 5398168.1044 - mae: 1758.0468 - val_loss: 13072443.0000 - val_mae: 2816.0334\n",
      "Epoch 328/350\n",
      "91/91 - 0s - loss: 5418526.3407 - mae: 1758.1587 - val_loss: 14383635.0000 - val_mae: 2914.5667\n",
      "Epoch 329/350\n",
      "91/91 - 0s - loss: 5468675.7967 - mae: 1786.9155 - val_loss: 13881856.0000 - val_mae: 2854.8633\n",
      "Epoch 330/350\n",
      "91/91 - 0s - loss: 5518138.1264 - mae: 1782.7258 - val_loss: 14135008.0000 - val_mae: 2895.6672\n",
      "Epoch 331/350\n",
      "91/91 - 0s - loss: 5604713.9725 - mae: 1859.1237 - val_loss: 13744232.0000 - val_mae: 2860.6418\n",
      "Epoch 332/350\n",
      "91/91 - 0s - loss: 5544338.2692 - mae: 1791.4938 - val_loss: 12997035.0000 - val_mae: 2826.7415\n",
      "Epoch 333/350\n",
      "91/91 - 0s - loss: 5475473.2253 - mae: 1818.4108 - val_loss: 14638604.0000 - val_mae: 2977.9153\n",
      "Epoch 334/350\n",
      "91/91 - 0s - loss: 5385234.1429 - mae: 1815.4478 - val_loss: 12800840.0000 - val_mae: 2768.9783\n",
      "Epoch 335/350\n",
      "91/91 - 0s - loss: 5626056.8297 - mae: 1844.8168 - val_loss: 13861264.0000 - val_mae: 2849.9319\n",
      "Epoch 336/350\n",
      "91/91 - 0s - loss: 5346337.7912 - mae: 1754.9695 - val_loss: 13076779.0000 - val_mae: 2809.1785\n",
      "Epoch 337/350\n",
      "91/91 - 0s - loss: 5338513.9780 - mae: 1762.7206 - val_loss: 13563232.0000 - val_mae: 2844.7585\n",
      "Epoch 338/350\n",
      "91/91 - 0s - loss: 5524761.0769 - mae: 1755.7373 - val_loss: 13277181.0000 - val_mae: 2858.3340\n",
      "Epoch 339/350\n",
      "91/91 - 0s - loss: 5869419.5659 - mae: 1905.3795 - val_loss: 14012911.0000 - val_mae: 2902.0830\n",
      "Epoch 340/350\n",
      "91/91 - 0s - loss: 5794564.5879 - mae: 1791.8876 - val_loss: 12969883.0000 - val_mae: 2941.6982\n",
      "Epoch 341/350\n",
      "91/91 - 0s - loss: 5388798.0330 - mae: 1767.5341 - val_loss: 17352094.0000 - val_mae: 3358.9534\n",
      "Epoch 342/350\n",
      "91/91 - 0s - loss: 6602405.0604 - mae: 2022.9189 - val_loss: 12884736.0000 - val_mae: 2947.6350\n",
      "Epoch 343/350\n",
      "91/91 - 0s - loss: 5993461.3077 - mae: 1882.9932 - val_loss: 16550149.0000 - val_mae: 3259.6472\n",
      "Epoch 344/350\n",
      "91/91 - 0s - loss: 5634428.9945 - mae: 1839.7601 - val_loss: 13092843.0000 - val_mae: 2968.0852\n",
      "Epoch 345/350\n",
      "91/91 - 0s - loss: 6120515.3022 - mae: 1852.1073 - val_loss: 14758223.0000 - val_mae: 3032.4763\n",
      "Epoch 346/350\n",
      "91/91 - 0s - loss: 5892155.1484 - mae: 1900.0747 - val_loss: 13060916.0000 - val_mae: 2868.8679\n",
      "Epoch 347/350\n",
      "91/91 - 0s - loss: 5457769.5495 - mae: 1804.0963 - val_loss: 14423248.0000 - val_mae: 2985.3035\n",
      "Epoch 348/350\n",
      "91/91 - 0s - loss: 5710373.0632 - mae: 1858.4197 - val_loss: 12856196.0000 - val_mae: 2887.0652\n",
      "Epoch 349/350\n",
      "91/91 - 0s - loss: 5663707.5385 - mae: 1803.4460 - val_loss: 13279773.0000 - val_mae: 2854.4802\n",
      "Epoch 350/350\n",
      "91/91 - 0s - loss: 5112422.0110 - mae: 1762.0989 - val_loss: 14381907.0000 - val_mae: 2965.9297\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fcebc2e48d0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the model\n",
    "model.fit(X_train, y_train, epochs=350, batch_size=32, verbose=2, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 14381907.000, RMSE: 3792.348, MAE: 14381907.000\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "mse, mae = model.evaluate(X_test, y_test, verbose=0)\n",
    "print('MSE: {0:.3f}, RMSE: {1:.3f}, MAE: {0:.3f}'.format(mse, np.sqrt(mse), mae))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a prediction\n",
    "row = np.asarray([18024.0, 16722.0, 14385.0, 21342.0, 17180.0]).reshape((1, n_steps, 1))\n",
    "yhat = model.predict(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: 16457.160\n"
     ]
    }
   ],
   "source": [
    "print('Predicted: {0:.3f}'.format(yhat[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
