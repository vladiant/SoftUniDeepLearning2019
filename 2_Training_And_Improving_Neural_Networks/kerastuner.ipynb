{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From https://www.tensorflow.org/tutorials/keras/keras_tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "import IPython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 139
    },
    "colab_type": "code",
    "id": "PWIVNPQkImWa",
    "outputId": "6a43753f-5af7-40c7-c9cf-c3f4e2c3740e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: You are using pip version 20.0.2; however, version 20.2.4 is available.\r\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "# !git clone https://github.com/keras-team/keras-tuner\n",
    "!pip install -q -U keras-tuner\n",
    "import kerastuner as kt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "8suuvgWIJJdn",
    "outputId": "2e729477-d9d9-4a9d-8c8f-6c3903b49c4a"
   },
   "source": [
    "Download and prepare the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "rpGxkRItJUTR",
    "outputId": "01a1d7c8-d4ae-4889-f7df-155eb9a51e90"
   },
   "outputs": [],
   "source": [
    "(img_train, label_train), (img_test, label_test) = keras.datasets.fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 905
    },
    "colab_type": "code",
    "id": "b3z4_6-PJaCg",
    "outputId": "4c1c6157-9e1c-4c57-c64b-2bce741a5471"
   },
   "outputs": [],
   "source": [
    "# Normalize pixel values between 0 and 1\n",
    "img_train = img_train.astype('float32') / 255.0\n",
    "img_test = img_test.astype('float32') / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the model\n",
    "When you build a model for hypertuning, you also define the hyperparameter search space in addition to the model architecture. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ee6JnDtiJeH1"
   },
   "outputs": [],
   "source": [
    "def model_builder(hp):\n",
    "  model = keras.Sequential()\n",
    "  model.add(keras.layers.Flatten(input_shape=(28, 28)))\n",
    "  \n",
    "  # Tune the number of units in the first Dense layer\n",
    "  # Choose an optimal value between 32-512\n",
    "  hp_units = hp.Int('units', min_value = 32, max_value = 512, step = 32)\n",
    "  model.add(keras.layers.Dense(units = hp_units, activation = 'relu'))\n",
    "  model.add(keras.layers.Dense(10))\n",
    "\n",
    "  # Tune the learning rate for the optimizer \n",
    "  # Choose an optimal value from 0.01, 0.001, or 0.0001\n",
    "  hp_learning_rate = hp.Choice('learning_rate', values = [1e-2, 1e-3, 1e-4]) \n",
    "  \n",
    "  model.compile(optimizer = keras.optimizers.Adam(learning_rate = hp_learning_rate),\n",
    "                loss = keras.losses.SparseCategoricalCrossentropy(from_logits = True), \n",
    "                metrics = ['accuracy'])\n",
    "  \n",
    "  return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZPgZYxTEJxFd"
   },
   "source": [
    "### Instantiate the tuner and perform hypertuning\n",
    "The Keras Tuner has four tuners available - RandomSearch, Hyperband, BayesianOptimization, and Sklearn. In this tutorial, you use the Hyperband tuner."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "y1aHFsslJ9XU"
   },
   "source": [
    "To instantiate the Hyperband tuner, you must specify the hypermodel, the objective to optimize and the maximum number of epochs to train (max_epochs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Xr79stzZKDZ_"
   },
   "outputs": [],
   "source": [
    "tuner = kt.Hyperband(model_builder,\n",
    "                     objective = 'val_accuracy', \n",
    "                     max_epochs = 10,\n",
    "                     factor = 3,\n",
    "                     directory = 'my_dir',\n",
    "                     project_name = 'intro_to_kt')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UziPhnNIKHBz"
   },
   "source": [
    "Callback to clear the training outputs at the end of every training step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClearTrainingOutput(tf.keras.callbacks.Callback):\n",
    "  def on_train_end(*args, **kwargs):\n",
    "    IPython.display.clear_output(wait = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the hyperparameter search. The arguments for the search method are the same as those used for tf.keras.model.fit in addition to the callback above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial complete</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial summary</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Trial ID: ec1c4d3032c3bbb221f585958d73c03d</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Score: 0.8766000270843506</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Best step: 0</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#7E57C2\"><h2 style=\"font-size:16px\">Hyperparameters:</h2></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-learning_rate: 0.0001</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-tuner/bracket: 0</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-tuner/epochs: 10</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-tuner/initial_epoch: 0</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-tuner/round: 0</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-units: 352</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Oracle triggered exit\n",
      "\n",
      "The hyperparameter search is complete. The optimal number of units in the first densely-connected\n",
      "layer is 384 and the optimal learning rate for the optimizer\n",
      "is 0.001.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tuner.search(img_train, label_train,\n",
    "             epochs = 10,\n",
    "             validation_data = (img_test, label_test),\n",
    "             callbacks = [ClearTrainingOutput()])\n",
    "\n",
    "# Get the optimal hyperparameters\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials = 1)[0]\n",
    "\n",
    "print(f\"\"\"\n",
    "The hyperparameter search is complete. The optimal number of units in the first densely-connected\n",
    "layer is {best_hps.get('units')} and the optimal learning rate for the optimizer\n",
    "is {best_hps.get('learning_rate')}.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrain the model with the optimal hyperparameters from the search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - ETA: 4:36 - loss: 2.2262 - accuracy: 0.18 - ETA: 12s - loss: 1.1662 - accuracy: 0.6021 - ETA: 7s - loss: 0.9771 - accuracy: 0.658 - ETA: 6s - loss: 0.8906 - accuracy: 0.68 - ETA: 5s - loss: 0.8344 - accuracy: 0.70 - ETA: 4s - loss: 0.7944 - accuracy: 0.72 - ETA: 4s - loss: 0.7617 - accuracy: 0.73 - ETA: 4s - loss: 0.7426 - accuracy: 0.74 - ETA: 3s - loss: 0.7197 - accuracy: 0.74 - ETA: 3s - loss: 0.6990 - accuracy: 0.75 - ETA: 3s - loss: 0.6804 - accuracy: 0.76 - ETA: 3s - loss: 0.6736 - accuracy: 0.76 - ETA: 3s - loss: 0.6601 - accuracy: 0.76 - ETA: 3s - loss: 0.6439 - accuracy: 0.77 - ETA: 2s - loss: 0.6286 - accuracy: 0.77 - ETA: 2s - loss: 0.6231 - accuracy: 0.78 - ETA: 2s - loss: 0.6179 - accuracy: 0.78 - ETA: 2s - loss: 0.6098 - accuracy: 0.78 - ETA: 2s - loss: 0.6036 - accuracy: 0.78 - ETA: 2s - loss: 0.5993 - accuracy: 0.78 - ETA: 2s - loss: 0.5909 - accuracy: 0.79 - ETA: 2s - loss: 0.5851 - accuracy: 0.79 - ETA: 2s - loss: 0.5801 - accuracy: 0.79 - ETA: 2s - loss: 0.5761 - accuracy: 0.79 - ETA: 2s - loss: 0.5699 - accuracy: 0.79 - ETA: 2s - loss: 0.5639 - accuracy: 0.80 - ETA: 2s - loss: 0.5586 - accuracy: 0.80 - ETA: 2s - loss: 0.5539 - accuracy: 0.80 - ETA: 1s - loss: 0.5545 - accuracy: 0.80 - ETA: 1s - loss: 0.5504 - accuracy: 0.80 - ETA: 1s - loss: 0.5468 - accuracy: 0.80 - ETA: 1s - loss: 0.5439 - accuracy: 0.80 - ETA: 1s - loss: 0.5396 - accuracy: 0.80 - ETA: 1s - loss: 0.5340 - accuracy: 0.81 - ETA: 1s - loss: 0.5317 - accuracy: 0.81 - ETA: 1s - loss: 0.5274 - accuracy: 0.81 - ETA: 1s - loss: 0.5252 - accuracy: 0.81 - ETA: 1s - loss: 0.5217 - accuracy: 0.81 - ETA: 1s - loss: 0.5184 - accuracy: 0.81 - ETA: 1s - loss: 0.5151 - accuracy: 0.81 - ETA: 1s - loss: 0.5116 - accuracy: 0.81 - ETA: 1s - loss: 0.5087 - accuracy: 0.81 - ETA: 1s - loss: 0.5081 - accuracy: 0.81 - ETA: 1s - loss: 0.5059 - accuracy: 0.81 - ETA: 0s - loss: 0.5026 - accuracy: 0.82 - ETA: 0s - loss: 0.5008 - accuracy: 0.82 - ETA: 0s - loss: 0.4994 - accuracy: 0.82 - ETA: 0s - loss: 0.4975 - accuracy: 0.82 - ETA: 0s - loss: 0.4957 - accuracy: 0.82 - ETA: 0s - loss: 0.4930 - accuracy: 0.82 - ETA: 0s - loss: 0.4917 - accuracy: 0.82 - ETA: 0s - loss: 0.4900 - accuracy: 0.82 - ETA: 0s - loss: 0.4880 - accuracy: 0.82 - ETA: 0s - loss: 0.4864 - accuracy: 0.82 - ETA: 0s - loss: 0.4848 - accuracy: 0.82 - ETA: 0s - loss: 0.4838 - accuracy: 0.82 - ETA: 0s - loss: 0.4819 - accuracy: 0.82 - ETA: 0s - loss: 0.4809 - accuracy: 0.82 - ETA: 0s - loss: 0.4803 - accuracy: 0.82 - ETA: 0s - loss: 0.4785 - accuracy: 0.82 - ETA: 0s - loss: 0.4775 - accuracy: 0.83 - 3s 58us/sample - loss: 0.4763 - accuracy: 0.8306 - val_loss: 0.4074 - val_accuracy: 0.8556\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - ETA: 5s - loss: 0.2937 - accuracy: 0.87 - ETA: 3s - loss: 0.3982 - accuracy: 0.84 - ETA: 3s - loss: 0.3838 - accuracy: 0.85 - ETA: 3s - loss: 0.3719 - accuracy: 0.86 - ETA: 2s - loss: 0.3687 - accuracy: 0.86 - ETA: 2s - loss: 0.3736 - accuracy: 0.86 - ETA: 2s - loss: 0.3728 - accuracy: 0.86 - ETA: 2s - loss: 0.3657 - accuracy: 0.86 - ETA: 2s - loss: 0.3688 - accuracy: 0.86 - ETA: 2s - loss: 0.3700 - accuracy: 0.86 - ETA: 2s - loss: 0.3682 - accuracy: 0.86 - ETA: 2s - loss: 0.3672 - accuracy: 0.86 - ETA: 2s - loss: 0.3678 - accuracy: 0.86 - ETA: 2s - loss: 0.3671 - accuracy: 0.86 - ETA: 2s - loss: 0.3646 - accuracy: 0.86 - ETA: 2s - loss: 0.3639 - accuracy: 0.86 - ETA: 2s - loss: 0.3634 - accuracy: 0.86 - ETA: 2s - loss: 0.3618 - accuracy: 0.86 - ETA: 2s - loss: 0.3618 - accuracy: 0.86 - ETA: 2s - loss: 0.3604 - accuracy: 0.86 - ETA: 1s - loss: 0.3603 - accuracy: 0.86 - ETA: 1s - loss: 0.3595 - accuracy: 0.86 - ETA: 1s - loss: 0.3591 - accuracy: 0.86 - ETA: 1s - loss: 0.3593 - accuracy: 0.86 - ETA: 1s - loss: 0.3602 - accuracy: 0.86 - ETA: 1s - loss: 0.3604 - accuracy: 0.86 - ETA: 1s - loss: 0.3604 - accuracy: 0.86 - ETA: 1s - loss: 0.3594 - accuracy: 0.86 - ETA: 1s - loss: 0.3588 - accuracy: 0.86 - ETA: 1s - loss: 0.3581 - accuracy: 0.86 - ETA: 1s - loss: 0.3573 - accuracy: 0.86 - ETA: 1s - loss: 0.3567 - accuracy: 0.86 - ETA: 1s - loss: 0.3574 - accuracy: 0.86 - ETA: 1s - loss: 0.3577 - accuracy: 0.86 - ETA: 1s - loss: 0.3578 - accuracy: 0.86 - ETA: 1s - loss: 0.3585 - accuracy: 0.86 - ETA: 1s - loss: 0.3574 - accuracy: 0.86 - ETA: 1s - loss: 0.3570 - accuracy: 0.86 - ETA: 1s - loss: 0.3575 - accuracy: 0.86 - ETA: 1s - loss: 0.3574 - accuracy: 0.86 - ETA: 1s - loss: 0.3574 - accuracy: 0.86 - ETA: 1s - loss: 0.3578 - accuracy: 0.86 - ETA: 0s - loss: 0.3576 - accuracy: 0.86 - ETA: 0s - loss: 0.3590 - accuracy: 0.86 - ETA: 0s - loss: 0.3588 - accuracy: 0.86 - ETA: 0s - loss: 0.3594 - accuracy: 0.86 - ETA: 0s - loss: 0.3596 - accuracy: 0.86 - ETA: 0s - loss: 0.3594 - accuracy: 0.86 - ETA: 0s - loss: 0.3593 - accuracy: 0.86 - ETA: 0s - loss: 0.3585 - accuracy: 0.86 - ETA: 0s - loss: 0.3585 - accuracy: 0.86 - ETA: 0s - loss: 0.3584 - accuracy: 0.86 - ETA: 0s - loss: 0.3574 - accuracy: 0.86 - ETA: 0s - loss: 0.3575 - accuracy: 0.86 - ETA: 0s - loss: 0.3573 - accuracy: 0.86 - ETA: 0s - loss: 0.3569 - accuracy: 0.86 - ETA: 0s - loss: 0.3572 - accuracy: 0.86 - ETA: 0s - loss: 0.3570 - accuracy: 0.86 - ETA: 0s - loss: 0.3567 - accuracy: 0.86 - ETA: 0s - loss: 0.3574 - accuracy: 0.86 - ETA: 0s - loss: 0.3574 - accuracy: 0.86 - ETA: 0s - loss: 0.3576 - accuracy: 0.86 - ETA: 0s - loss: 0.3575 - accuracy: 0.86 - 3s 57us/sample - loss: 0.3580 - accuracy: 0.8680 - val_loss: 0.3804 - val_accuracy: 0.8594\n",
      "Epoch 3/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000/60000 [==============================] - ETA: 5s - loss: 0.4724 - accuracy: 0.78 - ETA: 3s - loss: 0.3369 - accuracy: 0.87 - ETA: 3s - loss: 0.3689 - accuracy: 0.86 - ETA: 3s - loss: 0.3556 - accuracy: 0.86 - ETA: 3s - loss: 0.3538 - accuracy: 0.86 - ETA: 3s - loss: 0.3519 - accuracy: 0.86 - ETA: 3s - loss: 0.3479 - accuracy: 0.86 - ETA: 3s - loss: 0.3510 - accuracy: 0.86 - ETA: 3s - loss: 0.3492 - accuracy: 0.86 - ETA: 2s - loss: 0.3455 - accuracy: 0.87 - ETA: 3s - loss: 0.3483 - accuracy: 0.87 - ETA: 3s - loss: 0.3468 - accuracy: 0.87 - ETA: 3s - loss: 0.3464 - accuracy: 0.87 - ETA: 3s - loss: 0.3432 - accuracy: 0.87 - ETA: 3s - loss: 0.3403 - accuracy: 0.87 - ETA: 3s - loss: 0.3362 - accuracy: 0.87 - ETA: 2s - loss: 0.3359 - accuracy: 0.87 - ETA: 2s - loss: 0.3367 - accuracy: 0.87 - ETA: 2s - loss: 0.3351 - accuracy: 0.87 - ETA: 2s - loss: 0.3375 - accuracy: 0.87 - ETA: 2s - loss: 0.3377 - accuracy: 0.87 - ETA: 2s - loss: 0.3374 - accuracy: 0.87 - ETA: 2s - loss: 0.3372 - accuracy: 0.87 - ETA: 2s - loss: 0.3347 - accuracy: 0.87 - ETA: 2s - loss: 0.3334 - accuracy: 0.87 - ETA: 2s - loss: 0.3341 - accuracy: 0.87 - ETA: 2s - loss: 0.3331 - accuracy: 0.87 - ETA: 2s - loss: 0.3338 - accuracy: 0.87 - ETA: 2s - loss: 0.3339 - accuracy: 0.87 - ETA: 2s - loss: 0.3335 - accuracy: 0.87 - ETA: 2s - loss: 0.3336 - accuracy: 0.87 - ETA: 2s - loss: 0.3331 - accuracy: 0.87 - ETA: 2s - loss: 0.3320 - accuracy: 0.87 - ETA: 2s - loss: 0.3323 - accuracy: 0.87 - ETA: 2s - loss: 0.3318 - accuracy: 0.87 - ETA: 2s - loss: 0.3329 - accuracy: 0.87 - ETA: 1s - loss: 0.3332 - accuracy: 0.87 - ETA: 1s - loss: 0.3327 - accuracy: 0.87 - ETA: 1s - loss: 0.3331 - accuracy: 0.87 - ETA: 1s - loss: 0.3332 - accuracy: 0.87 - ETA: 1s - loss: 0.3325 - accuracy: 0.87 - ETA: 1s - loss: 0.3318 - accuracy: 0.87 - ETA: 1s - loss: 0.3321 - accuracy: 0.87 - ETA: 1s - loss: 0.3315 - accuracy: 0.87 - ETA: 1s - loss: 0.3317 - accuracy: 0.87 - ETA: 1s - loss: 0.3317 - accuracy: 0.87 - ETA: 1s - loss: 0.3316 - accuracy: 0.87 - ETA: 1s - loss: 0.3310 - accuracy: 0.87 - ETA: 1s - loss: 0.3314 - accuracy: 0.87 - ETA: 1s - loss: 0.3315 - accuracy: 0.87 - ETA: 1s - loss: 0.3303 - accuracy: 0.87 - ETA: 1s - loss: 0.3296 - accuracy: 0.87 - ETA: 1s - loss: 0.3293 - accuracy: 0.87 - ETA: 1s - loss: 0.3285 - accuracy: 0.87 - ETA: 1s - loss: 0.3287 - accuracy: 0.87 - ETA: 1s - loss: 0.3281 - accuracy: 0.87 - ETA: 0s - loss: 0.3276 - accuracy: 0.87 - ETA: 0s - loss: 0.3272 - accuracy: 0.87 - ETA: 0s - loss: 0.3266 - accuracy: 0.87 - ETA: 0s - loss: 0.3267 - accuracy: 0.87 - ETA: 0s - loss: 0.3265 - accuracy: 0.87 - ETA: 0s - loss: 0.3259 - accuracy: 0.88 - ETA: 0s - loss: 0.3252 - accuracy: 0.88 - ETA: 0s - loss: 0.3250 - accuracy: 0.88 - ETA: 0s - loss: 0.3243 - accuracy: 0.88 - ETA: 0s - loss: 0.3239 - accuracy: 0.88 - ETA: 0s - loss: 0.3239 - accuracy: 0.88 - ETA: 0s - loss: 0.3244 - accuracy: 0.88 - ETA: 0s - loss: 0.3246 - accuracy: 0.87 - ETA: 0s - loss: 0.3250 - accuracy: 0.87 - ETA: 0s - loss: 0.3251 - accuracy: 0.87 - ETA: 0s - loss: 0.3246 - accuracy: 0.87 - ETA: 0s - loss: 0.3237 - accuracy: 0.88 - ETA: 0s - loss: 0.3237 - accuracy: 0.88 - ETA: 0s - loss: 0.3232 - accuracy: 0.88 - ETA: 0s - loss: 0.3236 - accuracy: 0.88 - ETA: 0s - loss: 0.3237 - accuracy: 0.87 - 4s 69us/sample - loss: 0.3239 - accuracy: 0.8799 - val_loss: 0.3582 - val_accuracy: 0.8714\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - ETA: 5s - loss: 0.2559 - accuracy: 0.90 - ETA: 4s - loss: 0.2848 - accuracy: 0.89 - ETA: 4s - loss: 0.2716 - accuracy: 0.89 - ETA: 4s - loss: 0.2896 - accuracy: 0.89 - ETA: 3s - loss: 0.2929 - accuracy: 0.89 - ETA: 3s - loss: 0.2933 - accuracy: 0.89 - ETA: 3s - loss: 0.2990 - accuracy: 0.89 - ETA: 3s - loss: 0.3051 - accuracy: 0.89 - ETA: 3s - loss: 0.3060 - accuracy: 0.88 - ETA: 3s - loss: 0.3056 - accuracy: 0.89 - ETA: 3s - loss: 0.3042 - accuracy: 0.89 - ETA: 3s - loss: 0.2995 - accuracy: 0.89 - ETA: 3s - loss: 0.2979 - accuracy: 0.89 - ETA: 3s - loss: 0.2940 - accuracy: 0.89 - ETA: 3s - loss: 0.2956 - accuracy: 0.89 - ETA: 3s - loss: 0.2940 - accuracy: 0.89 - ETA: 2s - loss: 0.2914 - accuracy: 0.89 - ETA: 2s - loss: 0.2940 - accuracy: 0.89 - ETA: 2s - loss: 0.2945 - accuracy: 0.89 - ETA: 2s - loss: 0.2956 - accuracy: 0.89 - ETA: 2s - loss: 0.2951 - accuracy: 0.89 - ETA: 2s - loss: 0.2970 - accuracy: 0.89 - ETA: 2s - loss: 0.2973 - accuracy: 0.88 - ETA: 2s - loss: 0.2971 - accuracy: 0.88 - ETA: 2s - loss: 0.2972 - accuracy: 0.88 - ETA: 2s - loss: 0.2991 - accuracy: 0.88 - ETA: 2s - loss: 0.2985 - accuracy: 0.88 - ETA: 2s - loss: 0.2993 - accuracy: 0.88 - ETA: 2s - loss: 0.2984 - accuracy: 0.88 - ETA: 2s - loss: 0.2982 - accuracy: 0.88 - ETA: 2s - loss: 0.2974 - accuracy: 0.88 - ETA: 2s - loss: 0.2968 - accuracy: 0.88 - ETA: 2s - loss: 0.2955 - accuracy: 0.88 - ETA: 2s - loss: 0.2962 - accuracy: 0.88 - ETA: 2s - loss: 0.2959 - accuracy: 0.88 - ETA: 2s - loss: 0.2957 - accuracy: 0.88 - ETA: 2s - loss: 0.2960 - accuracy: 0.88 - ETA: 1s - loss: 0.2956 - accuracy: 0.89 - ETA: 1s - loss: 0.2961 - accuracy: 0.89 - ETA: 1s - loss: 0.2961 - accuracy: 0.89 - ETA: 1s - loss: 0.2961 - accuracy: 0.89 - ETA: 1s - loss: 0.2969 - accuracy: 0.89 - ETA: 1s - loss: 0.2969 - accuracy: 0.89 - ETA: 1s - loss: 0.2966 - accuracy: 0.89 - ETA: 1s - loss: 0.2954 - accuracy: 0.89 - ETA: 1s - loss: 0.2950 - accuracy: 0.89 - ETA: 1s - loss: 0.2947 - accuracy: 0.89 - ETA: 1s - loss: 0.2953 - accuracy: 0.89 - ETA: 1s - loss: 0.2950 - accuracy: 0.89 - ETA: 1s - loss: 0.2956 - accuracy: 0.89 - ETA: 1s - loss: 0.2949 - accuracy: 0.89 - ETA: 1s - loss: 0.2947 - accuracy: 0.89 - ETA: 1s - loss: 0.2949 - accuracy: 0.89 - ETA: 1s - loss: 0.2950 - accuracy: 0.89 - ETA: 1s - loss: 0.2959 - accuracy: 0.89 - ETA: 1s - loss: 0.2963 - accuracy: 0.89 - ETA: 1s - loss: 0.2964 - accuracy: 0.89 - ETA: 1s - loss: 0.2961 - accuracy: 0.89 - ETA: 0s - loss: 0.2964 - accuracy: 0.89 - ETA: 0s - loss: 0.2969 - accuracy: 0.88 - ETA: 0s - loss: 0.2969 - accuracy: 0.89 - ETA: 0s - loss: 0.2960 - accuracy: 0.89 - ETA: 0s - loss: 0.2969 - accuracy: 0.89 - ETA: 0s - loss: 0.2974 - accuracy: 0.89 - ETA: 0s - loss: 0.2974 - accuracy: 0.89 - ETA: 0s - loss: 0.2978 - accuracy: 0.89 - ETA: 0s - loss: 0.2983 - accuracy: 0.88 - ETA: 0s - loss: 0.2986 - accuracy: 0.88 - ETA: 0s - loss: 0.2990 - accuracy: 0.88 - ETA: 0s - loss: 0.2992 - accuracy: 0.88 - ETA: 0s - loss: 0.2989 - accuracy: 0.88 - ETA: 0s - loss: 0.2990 - accuracy: 0.88 - ETA: 0s - loss: 0.2986 - accuracy: 0.88 - ETA: 0s - loss: 0.2981 - accuracy: 0.88 - ETA: 0s - loss: 0.2983 - accuracy: 0.88 - 4s 68us/sample - loss: 0.2985 - accuracy: 0.8893 - val_loss: 0.3789 - val_accuracy: 0.8644\n",
      "Epoch 5/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000/60000 [==============================] - ETA: 5s - loss: 0.2218 - accuracy: 0.90 - ETA: 4s - loss: 0.2563 - accuracy: 0.90 - ETA: 3s - loss: 0.2504 - accuracy: 0.91 - ETA: 3s - loss: 0.2665 - accuracy: 0.90 - ETA: 3s - loss: 0.2717 - accuracy: 0.90 - ETA: 3s - loss: 0.2737 - accuracy: 0.89 - ETA: 3s - loss: 0.2753 - accuracy: 0.89 - ETA: 3s - loss: 0.2773 - accuracy: 0.89 - ETA: 3s - loss: 0.2794 - accuracy: 0.89 - ETA: 3s - loss: 0.2763 - accuracy: 0.89 - ETA: 3s - loss: 0.2772 - accuracy: 0.89 - ETA: 2s - loss: 0.2796 - accuracy: 0.89 - ETA: 2s - loss: 0.2805 - accuracy: 0.89 - ETA: 2s - loss: 0.2791 - accuracy: 0.89 - ETA: 2s - loss: 0.2787 - accuracy: 0.89 - ETA: 2s - loss: 0.2803 - accuracy: 0.89 - ETA: 2s - loss: 0.2782 - accuracy: 0.89 - ETA: 2s - loss: 0.2801 - accuracy: 0.89 - ETA: 2s - loss: 0.2788 - accuracy: 0.89 - ETA: 2s - loss: 0.2782 - accuracy: 0.89 - ETA: 2s - loss: 0.2778 - accuracy: 0.89 - ETA: 2s - loss: 0.2782 - accuracy: 0.89 - ETA: 2s - loss: 0.2786 - accuracy: 0.89 - ETA: 2s - loss: 0.2785 - accuracy: 0.89 - ETA: 2s - loss: 0.2786 - accuracy: 0.89 - ETA: 2s - loss: 0.2794 - accuracy: 0.89 - ETA: 2s - loss: 0.2797 - accuracy: 0.89 - ETA: 2s - loss: 0.2804 - accuracy: 0.89 - ETA: 2s - loss: 0.2805 - accuracy: 0.89 - ETA: 2s - loss: 0.2805 - accuracy: 0.89 - ETA: 1s - loss: 0.2805 - accuracy: 0.89 - ETA: 1s - loss: 0.2802 - accuracy: 0.89 - ETA: 1s - loss: 0.2804 - accuracy: 0.89 - ETA: 1s - loss: 0.2798 - accuracy: 0.89 - ETA: 1s - loss: 0.2806 - accuracy: 0.89 - ETA: 1s - loss: 0.2801 - accuracy: 0.89 - ETA: 1s - loss: 0.2802 - accuracy: 0.89 - ETA: 1s - loss: 0.2807 - accuracy: 0.89 - ETA: 1s - loss: 0.2807 - accuracy: 0.89 - ETA: 1s - loss: 0.2804 - accuracy: 0.89 - ETA: 1s - loss: 0.2800 - accuracy: 0.89 - ETA: 1s - loss: 0.2795 - accuracy: 0.89 - ETA: 1s - loss: 0.2788 - accuracy: 0.89 - ETA: 1s - loss: 0.2784 - accuracy: 0.89 - ETA: 1s - loss: 0.2781 - accuracy: 0.89 - ETA: 1s - loss: 0.2787 - accuracy: 0.89 - ETA: 1s - loss: 0.2785 - accuracy: 0.89 - ETA: 1s - loss: 0.2788 - accuracy: 0.89 - ETA: 1s - loss: 0.2793 - accuracy: 0.89 - ETA: 1s - loss: 0.2790 - accuracy: 0.89 - ETA: 1s - loss: 0.2794 - accuracy: 0.89 - ETA: 0s - loss: 0.2791 - accuracy: 0.89 - ETA: 0s - loss: 0.2794 - accuracy: 0.89 - ETA: 0s - loss: 0.2800 - accuracy: 0.89 - ETA: 0s - loss: 0.2801 - accuracy: 0.89 - ETA: 0s - loss: 0.2799 - accuracy: 0.89 - ETA: 0s - loss: 0.2801 - accuracy: 0.89 - ETA: 0s - loss: 0.2807 - accuracy: 0.89 - ETA: 0s - loss: 0.2810 - accuracy: 0.89 - ETA: 0s - loss: 0.2809 - accuracy: 0.89 - ETA: 0s - loss: 0.2813 - accuracy: 0.89 - ETA: 0s - loss: 0.2808 - accuracy: 0.89 - ETA: 0s - loss: 0.2809 - accuracy: 0.89 - ETA: 0s - loss: 0.2815 - accuracy: 0.89 - ETA: 0s - loss: 0.2813 - accuracy: 0.89 - ETA: 0s - loss: 0.2810 - accuracy: 0.89 - ETA: 0s - loss: 0.2812 - accuracy: 0.89 - ETA: 0s - loss: 0.2807 - accuracy: 0.89 - ETA: 0s - loss: 0.2806 - accuracy: 0.89 - ETA: 0s - loss: 0.2805 - accuracy: 0.89 - ETA: 0s - loss: 0.2801 - accuracy: 0.89 - 4s 64us/sample - loss: 0.2801 - accuracy: 0.8956 - val_loss: 0.3428 - val_accuracy: 0.8764\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - ETA: 5s - loss: 0.1633 - accuracy: 0.93 - ETA: 3s - loss: 0.2610 - accuracy: 0.90 - ETA: 3s - loss: 0.2742 - accuracy: 0.89 - ETA: 3s - loss: 0.2769 - accuracy: 0.89 - ETA: 3s - loss: 0.2675 - accuracy: 0.90 - ETA: 3s - loss: 0.2624 - accuracy: 0.90 - ETA: 3s - loss: 0.2665 - accuracy: 0.89 - ETA: 2s - loss: 0.2662 - accuracy: 0.89 - ETA: 2s - loss: 0.2641 - accuracy: 0.89 - ETA: 2s - loss: 0.2656 - accuracy: 0.89 - ETA: 2s - loss: 0.2632 - accuracy: 0.90 - ETA: 2s - loss: 0.2642 - accuracy: 0.90 - ETA: 2s - loss: 0.2616 - accuracy: 0.90 - ETA: 2s - loss: 0.2619 - accuracy: 0.90 - ETA: 2s - loss: 0.2661 - accuracy: 0.89 - ETA: 2s - loss: 0.2650 - accuracy: 0.90 - ETA: 2s - loss: 0.2632 - accuracy: 0.90 - ETA: 2s - loss: 0.2614 - accuracy: 0.90 - ETA: 2s - loss: 0.2613 - accuracy: 0.90 - ETA: 2s - loss: 0.2611 - accuracy: 0.90 - ETA: 2s - loss: 0.2620 - accuracy: 0.90 - ETA: 2s - loss: 0.2623 - accuracy: 0.90 - ETA: 2s - loss: 0.2623 - accuracy: 0.90 - ETA: 2s - loss: 0.2624 - accuracy: 0.90 - ETA: 2s - loss: 0.2628 - accuracy: 0.90 - ETA: 2s - loss: 0.2619 - accuracy: 0.90 - ETA: 2s - loss: 0.2621 - accuracy: 0.90 - ETA: 2s - loss: 0.2614 - accuracy: 0.90 - ETA: 2s - loss: 0.2620 - accuracy: 0.90 - ETA: 2s - loss: 0.2625 - accuracy: 0.90 - ETA: 2s - loss: 0.2620 - accuracy: 0.90 - ETA: 1s - loss: 0.2615 - accuracy: 0.90 - ETA: 1s - loss: 0.2605 - accuracy: 0.90 - ETA: 1s - loss: 0.2596 - accuracy: 0.90 - ETA: 1s - loss: 0.2596 - accuracy: 0.90 - ETA: 1s - loss: 0.2604 - accuracy: 0.90 - ETA: 1s - loss: 0.2607 - accuracy: 0.90 - ETA: 1s - loss: 0.2608 - accuracy: 0.90 - ETA: 1s - loss: 0.2616 - accuracy: 0.90 - ETA: 1s - loss: 0.2632 - accuracy: 0.90 - ETA: 1s - loss: 0.2632 - accuracy: 0.90 - ETA: 1s - loss: 0.2628 - accuracy: 0.90 - ETA: 1s - loss: 0.2635 - accuracy: 0.90 - ETA: 1s - loss: 0.2646 - accuracy: 0.90 - ETA: 1s - loss: 0.2643 - accuracy: 0.90 - ETA: 1s - loss: 0.2637 - accuracy: 0.90 - ETA: 1s - loss: 0.2634 - accuracy: 0.90 - ETA: 1s - loss: 0.2638 - accuracy: 0.90 - ETA: 1s - loss: 0.2638 - accuracy: 0.90 - ETA: 1s - loss: 0.2645 - accuracy: 0.90 - ETA: 0s - loss: 0.2651 - accuracy: 0.90 - ETA: 0s - loss: 0.2649 - accuracy: 0.90 - ETA: 0s - loss: 0.2652 - accuracy: 0.90 - ETA: 0s - loss: 0.2641 - accuracy: 0.90 - ETA: 0s - loss: 0.2647 - accuracy: 0.90 - ETA: 0s - loss: 0.2652 - accuracy: 0.90 - ETA: 0s - loss: 0.2655 - accuracy: 0.90 - ETA: 0s - loss: 0.2655 - accuracy: 0.90 - ETA: 0s - loss: 0.2651 - accuracy: 0.90 - ETA: 0s - loss: 0.2656 - accuracy: 0.90 - ETA: 0s - loss: 0.2653 - accuracy: 0.90 - ETA: 0s - loss: 0.2647 - accuracy: 0.90 - ETA: 0s - loss: 0.2644 - accuracy: 0.90 - ETA: 0s - loss: 0.2649 - accuracy: 0.90 - ETA: 0s - loss: 0.2649 - accuracy: 0.90 - ETA: 0s - loss: 0.2648 - accuracy: 0.90 - ETA: 0s - loss: 0.2649 - accuracy: 0.90 - ETA: 0s - loss: 0.2649 - accuracy: 0.90 - ETA: 0s - loss: 0.2652 - accuracy: 0.90 - ETA: 0s - loss: 0.2654 - accuracy: 0.90 - ETA: 0s - loss: 0.2649 - accuracy: 0.90 - ETA: 0s - loss: 0.2646 - accuracy: 0.90 - ETA: 0s - loss: 0.2647 - accuracy: 0.90 - 4s 66us/sample - loss: 0.2651 - accuracy: 0.9004 - val_loss: 0.3461 - val_accuracy: 0.8764\n",
      "Epoch 7/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000/60000 [==============================] - ETA: 5s - loss: 0.2921 - accuracy: 0.84 - ETA: 3s - loss: 0.2695 - accuracy: 0.89 - ETA: 3s - loss: 0.2469 - accuracy: 0.90 - ETA: 3s - loss: 0.2549 - accuracy: 0.90 - ETA: 3s - loss: 0.2490 - accuracy: 0.90 - ETA: 3s - loss: 0.2446 - accuracy: 0.90 - ETA: 3s - loss: 0.2427 - accuracy: 0.90 - ETA: 3s - loss: 0.2408 - accuracy: 0.90 - ETA: 3s - loss: 0.2400 - accuracy: 0.90 - ETA: 2s - loss: 0.2405 - accuracy: 0.90 - ETA: 2s - loss: 0.2448 - accuracy: 0.90 - ETA: 2s - loss: 0.2494 - accuracy: 0.90 - ETA: 2s - loss: 0.2503 - accuracy: 0.90 - ETA: 2s - loss: 0.2496 - accuracy: 0.90 - ETA: 2s - loss: 0.2499 - accuracy: 0.90 - ETA: 2s - loss: 0.2482 - accuracy: 0.90 - ETA: 2s - loss: 0.2465 - accuracy: 0.90 - ETA: 2s - loss: 0.2467 - accuracy: 0.90 - ETA: 2s - loss: 0.2461 - accuracy: 0.90 - ETA: 2s - loss: 0.2484 - accuracy: 0.90 - ETA: 2s - loss: 0.2484 - accuracy: 0.90 - ETA: 2s - loss: 0.2493 - accuracy: 0.90 - ETA: 2s - loss: 0.2493 - accuracy: 0.90 - ETA: 2s - loss: 0.2500 - accuracy: 0.90 - ETA: 2s - loss: 0.2483 - accuracy: 0.90 - ETA: 2s - loss: 0.2481 - accuracy: 0.90 - ETA: 2s - loss: 0.2485 - accuracy: 0.90 - ETA: 2s - loss: 0.2473 - accuracy: 0.90 - ETA: 2s - loss: 0.2474 - accuracy: 0.90 - ETA: 2s - loss: 0.2484 - accuracy: 0.90 - ETA: 2s - loss: 0.2488 - accuracy: 0.90 - ETA: 2s - loss: 0.2480 - accuracy: 0.90 - ETA: 2s - loss: 0.2493 - accuracy: 0.90 - ETA: 1s - loss: 0.2493 - accuracy: 0.90 - ETA: 1s - loss: 0.2492 - accuracy: 0.90 - ETA: 1s - loss: 0.2492 - accuracy: 0.90 - ETA: 1s - loss: 0.2493 - accuracy: 0.90 - ETA: 1s - loss: 0.2496 - accuracy: 0.90 - ETA: 1s - loss: 0.2498 - accuracy: 0.90 - ETA: 1s - loss: 0.2496 - accuracy: 0.90 - ETA: 1s - loss: 0.2506 - accuracy: 0.90 - ETA: 1s - loss: 0.2504 - accuracy: 0.90 - ETA: 1s - loss: 0.2509 - accuracy: 0.90 - ETA: 1s - loss: 0.2502 - accuracy: 0.90 - ETA: 1s - loss: 0.2504 - accuracy: 0.90 - ETA: 1s - loss: 0.2510 - accuracy: 0.90 - ETA: 1s - loss: 0.2504 - accuracy: 0.90 - ETA: 1s - loss: 0.2508 - accuracy: 0.90 - ETA: 1s - loss: 0.2515 - accuracy: 0.90 - ETA: 1s - loss: 0.2524 - accuracy: 0.90 - ETA: 1s - loss: 0.2530 - accuracy: 0.90 - ETA: 1s - loss: 0.2526 - accuracy: 0.90 - ETA: 1s - loss: 0.2530 - accuracy: 0.90 - ETA: 1s - loss: 0.2526 - accuracy: 0.90 - ETA: 0s - loss: 0.2535 - accuracy: 0.90 - ETA: 0s - loss: 0.2537 - accuracy: 0.90 - ETA: 0s - loss: 0.2541 - accuracy: 0.90 - ETA: 0s - loss: 0.2537 - accuracy: 0.90 - ETA: 0s - loss: 0.2533 - accuracy: 0.90 - ETA: 0s - loss: 0.2534 - accuracy: 0.90 - ETA: 0s - loss: 0.2535 - accuracy: 0.90 - ETA: 0s - loss: 0.2532 - accuracy: 0.90 - ETA: 0s - loss: 0.2533 - accuracy: 0.90 - ETA: 0s - loss: 0.2537 - accuracy: 0.90 - ETA: 0s - loss: 0.2535 - accuracy: 0.90 - ETA: 0s - loss: 0.2531 - accuracy: 0.90 - ETA: 0s - loss: 0.2529 - accuracy: 0.90 - ETA: 0s - loss: 0.2531 - accuracy: 0.90 - ETA: 0s - loss: 0.2533 - accuracy: 0.90 - ETA: 0s - loss: 0.2533 - accuracy: 0.90 - ETA: 0s - loss: 0.2533 - accuracy: 0.90 - ETA: 0s - loss: 0.2533 - accuracy: 0.90 - ETA: 0s - loss: 0.2533 - accuracy: 0.90 - 4s 66us/sample - loss: 0.2524 - accuracy: 0.9051 - val_loss: 0.3508 - val_accuracy: 0.8790\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - ETA: 5s - loss: 0.2230 - accuracy: 0.90 - ETA: 3s - loss: 0.2172 - accuracy: 0.91 - ETA: 3s - loss: 0.2131 - accuracy: 0.91 - ETA: 3s - loss: 0.2275 - accuracy: 0.91 - ETA: 3s - loss: 0.2258 - accuracy: 0.91 - ETA: 3s - loss: 0.2299 - accuracy: 0.91 - ETA: 3s - loss: 0.2264 - accuracy: 0.91 - ETA: 3s - loss: 0.2270 - accuracy: 0.91 - ETA: 3s - loss: 0.2274 - accuracy: 0.91 - ETA: 3s - loss: 0.2282 - accuracy: 0.91 - ETA: 3s - loss: 0.2304 - accuracy: 0.91 - ETA: 3s - loss: 0.2343 - accuracy: 0.91 - ETA: 3s - loss: 0.2340 - accuracy: 0.91 - ETA: 3s - loss: 0.2375 - accuracy: 0.91 - ETA: 3s - loss: 0.2417 - accuracy: 0.90 - ETA: 3s - loss: 0.2425 - accuracy: 0.90 - ETA: 2s - loss: 0.2416 - accuracy: 0.90 - ETA: 2s - loss: 0.2431 - accuracy: 0.90 - ETA: 2s - loss: 0.2443 - accuracy: 0.90 - ETA: 2s - loss: 0.2428 - accuracy: 0.90 - ETA: 2s - loss: 0.2434 - accuracy: 0.90 - ETA: 2s - loss: 0.2428 - accuracy: 0.90 - ETA: 2s - loss: 0.2446 - accuracy: 0.90 - ETA: 2s - loss: 0.2448 - accuracy: 0.90 - ETA: 2s - loss: 0.2453 - accuracy: 0.90 - ETA: 2s - loss: 0.2443 - accuracy: 0.90 - ETA: 2s - loss: 0.2441 - accuracy: 0.90 - ETA: 2s - loss: 0.2449 - accuracy: 0.90 - ETA: 2s - loss: 0.2447 - accuracy: 0.90 - ETA: 2s - loss: 0.2461 - accuracy: 0.90 - ETA: 2s - loss: 0.2459 - accuracy: 0.90 - ETA: 2s - loss: 0.2451 - accuracy: 0.90 - ETA: 2s - loss: 0.2454 - accuracy: 0.90 - ETA: 2s - loss: 0.2444 - accuracy: 0.90 - ETA: 1s - loss: 0.2450 - accuracy: 0.90 - ETA: 1s - loss: 0.2447 - accuracy: 0.90 - ETA: 1s - loss: 0.2441 - accuracy: 0.90 - ETA: 1s - loss: 0.2439 - accuracy: 0.90 - ETA: 1s - loss: 0.2431 - accuracy: 0.90 - ETA: 1s - loss: 0.2434 - accuracy: 0.90 - ETA: 1s - loss: 0.2434 - accuracy: 0.90 - ETA: 1s - loss: 0.2430 - accuracy: 0.90 - ETA: 1s - loss: 0.2427 - accuracy: 0.90 - ETA: 1s - loss: 0.2437 - accuracy: 0.90 - ETA: 1s - loss: 0.2444 - accuracy: 0.90 - ETA: 1s - loss: 0.2442 - accuracy: 0.90 - ETA: 1s - loss: 0.2441 - accuracy: 0.90 - ETA: 1s - loss: 0.2449 - accuracy: 0.90 - ETA: 1s - loss: 0.2449 - accuracy: 0.90 - ETA: 1s - loss: 0.2448 - accuracy: 0.90 - ETA: 1s - loss: 0.2438 - accuracy: 0.90 - ETA: 1s - loss: 0.2435 - accuracy: 0.90 - ETA: 1s - loss: 0.2441 - accuracy: 0.90 - ETA: 1s - loss: 0.2436 - accuracy: 0.90 - ETA: 1s - loss: 0.2428 - accuracy: 0.90 - ETA: 1s - loss: 0.2424 - accuracy: 0.90 - ETA: 1s - loss: 0.2426 - accuracy: 0.90 - ETA: 0s - loss: 0.2435 - accuracy: 0.90 - ETA: 0s - loss: 0.2435 - accuracy: 0.90 - ETA: 0s - loss: 0.2433 - accuracy: 0.90 - ETA: 0s - loss: 0.2434 - accuracy: 0.90 - ETA: 0s - loss: 0.2428 - accuracy: 0.90 - ETA: 0s - loss: 0.2428 - accuracy: 0.90 - ETA: 0s - loss: 0.2426 - accuracy: 0.90 - ETA: 0s - loss: 0.2423 - accuracy: 0.90 - ETA: 0s - loss: 0.2423 - accuracy: 0.90 - ETA: 0s - loss: 0.2422 - accuracy: 0.90 - ETA: 0s - loss: 0.2421 - accuracy: 0.90 - ETA: 0s - loss: 0.2419 - accuracy: 0.91 - ETA: 0s - loss: 0.2420 - accuracy: 0.91 - ETA: 0s - loss: 0.2414 - accuracy: 0.91 - ETA: 0s - loss: 0.2414 - accuracy: 0.91 - ETA: 0s - loss: 0.2414 - accuracy: 0.91 - ETA: 0s - loss: 0.2415 - accuracy: 0.91 - ETA: 0s - loss: 0.2416 - accuracy: 0.91 - ETA: 0s - loss: 0.2415 - accuracy: 0.91 - 4s 69us/sample - loss: 0.2415 - accuracy: 0.9105 - val_loss: 0.3391 - val_accuracy: 0.8858\n",
      "Epoch 9/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000/60000 [==============================] - ETA: 4s - loss: 0.2404 - accuracy: 0.90 - ETA: 3s - loss: 0.2466 - accuracy: 0.90 - ETA: 3s - loss: 0.2200 - accuracy: 0.91 - ETA: 3s - loss: 0.2199 - accuracy: 0.91 - ETA: 3s - loss: 0.2321 - accuracy: 0.91 - ETA: 3s - loss: 0.2352 - accuracy: 0.91 - ETA: 3s - loss: 0.2320 - accuracy: 0.91 - ETA: 3s - loss: 0.2298 - accuracy: 0.91 - ETA: 3s - loss: 0.2313 - accuracy: 0.91 - ETA: 3s - loss: 0.2336 - accuracy: 0.91 - ETA: 3s - loss: 0.2363 - accuracy: 0.91 - ETA: 3s - loss: 0.2350 - accuracy: 0.91 - ETA: 3s - loss: 0.2318 - accuracy: 0.91 - ETA: 3s - loss: 0.2339 - accuracy: 0.91 - ETA: 3s - loss: 0.2332 - accuracy: 0.91 - ETA: 2s - loss: 0.2317 - accuracy: 0.91 - ETA: 2s - loss: 0.2306 - accuracy: 0.91 - ETA: 2s - loss: 0.2303 - accuracy: 0.91 - ETA: 2s - loss: 0.2318 - accuracy: 0.91 - ETA: 2s - loss: 0.2321 - accuracy: 0.91 - ETA: 2s - loss: 0.2322 - accuracy: 0.91 - ETA: 2s - loss: 0.2323 - accuracy: 0.91 - ETA: 2s - loss: 0.2333 - accuracy: 0.91 - ETA: 2s - loss: 0.2323 - accuracy: 0.91 - ETA: 2s - loss: 0.2302 - accuracy: 0.91 - ETA: 2s - loss: 0.2296 - accuracy: 0.91 - ETA: 2s - loss: 0.2289 - accuracy: 0.91 - ETA: 2s - loss: 0.2296 - accuracy: 0.91 - ETA: 2s - loss: 0.2305 - accuracy: 0.91 - ETA: 2s - loss: 0.2320 - accuracy: 0.91 - ETA: 2s - loss: 0.2329 - accuracy: 0.91 - ETA: 2s - loss: 0.2331 - accuracy: 0.91 - ETA: 2s - loss: 0.2339 - accuracy: 0.91 - ETA: 2s - loss: 0.2347 - accuracy: 0.91 - ETA: 2s - loss: 0.2345 - accuracy: 0.91 - ETA: 2s - loss: 0.2352 - accuracy: 0.91 - ETA: 2s - loss: 0.2350 - accuracy: 0.91 - ETA: 2s - loss: 0.2335 - accuracy: 0.91 - ETA: 1s - loss: 0.2336 - accuracy: 0.91 - ETA: 1s - loss: 0.2340 - accuracy: 0.91 - ETA: 1s - loss: 0.2337 - accuracy: 0.91 - ETA: 1s - loss: 0.2335 - accuracy: 0.91 - ETA: 1s - loss: 0.2329 - accuracy: 0.91 - ETA: 1s - loss: 0.2319 - accuracy: 0.91 - ETA: 1s - loss: 0.2319 - accuracy: 0.91 - ETA: 1s - loss: 0.2328 - accuracy: 0.91 - ETA: 1s - loss: 0.2326 - accuracy: 0.91 - ETA: 1s - loss: 0.2324 - accuracy: 0.91 - ETA: 1s - loss: 0.2329 - accuracy: 0.91 - ETA: 1s - loss: 0.2321 - accuracy: 0.91 - ETA: 1s - loss: 0.2323 - accuracy: 0.91 - ETA: 1s - loss: 0.2321 - accuracy: 0.91 - ETA: 1s - loss: 0.2318 - accuracy: 0.91 - ETA: 1s - loss: 0.2324 - accuracy: 0.91 - ETA: 1s - loss: 0.2330 - accuracy: 0.91 - ETA: 1s - loss: 0.2335 - accuracy: 0.91 - ETA: 1s - loss: 0.2339 - accuracy: 0.91 - ETA: 0s - loss: 0.2336 - accuracy: 0.91 - ETA: 0s - loss: 0.2333 - accuracy: 0.91 - ETA: 0s - loss: 0.2326 - accuracy: 0.91 - ETA: 0s - loss: 0.2330 - accuracy: 0.91 - ETA: 0s - loss: 0.2338 - accuracy: 0.91 - ETA: 0s - loss: 0.2334 - accuracy: 0.91 - ETA: 0s - loss: 0.2331 - accuracy: 0.91 - ETA: 0s - loss: 0.2328 - accuracy: 0.91 - ETA: 0s - loss: 0.2329 - accuracy: 0.91 - ETA: 0s - loss: 0.2326 - accuracy: 0.91 - ETA: 0s - loss: 0.2328 - accuracy: 0.91 - ETA: 0s - loss: 0.2329 - accuracy: 0.91 - ETA: 0s - loss: 0.2324 - accuracy: 0.91 - ETA: 0s - loss: 0.2321 - accuracy: 0.91 - ETA: 0s - loss: 0.2321 - accuracy: 0.91 - ETA: 0s - loss: 0.2327 - accuracy: 0.91 - ETA: 0s - loss: 0.2323 - accuracy: 0.91 - 4s 66us/sample - loss: 0.2321 - accuracy: 0.9141 - val_loss: 0.3285 - val_accuracy: 0.8859\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - ETA: 5s - loss: 0.2728 - accuracy: 0.90 - ETA: 3s - loss: 0.2000 - accuracy: 0.92 - ETA: 3s - loss: 0.2175 - accuracy: 0.91 - ETA: 3s - loss: 0.2082 - accuracy: 0.92 - ETA: 3s - loss: 0.2124 - accuracy: 0.92 - ETA: 3s - loss: 0.2175 - accuracy: 0.91 - ETA: 3s - loss: 0.2138 - accuracy: 0.91 - ETA: 3s - loss: 0.2164 - accuracy: 0.91 - ETA: 2s - loss: 0.2141 - accuracy: 0.91 - ETA: 2s - loss: 0.2186 - accuracy: 0.91 - ETA: 2s - loss: 0.2175 - accuracy: 0.91 - ETA: 2s - loss: 0.2189 - accuracy: 0.91 - ETA: 2s - loss: 0.2198 - accuracy: 0.91 - ETA: 2s - loss: 0.2192 - accuracy: 0.91 - ETA: 2s - loss: 0.2195 - accuracy: 0.91 - ETA: 2s - loss: 0.2218 - accuracy: 0.91 - ETA: 2s - loss: 0.2194 - accuracy: 0.91 - ETA: 2s - loss: 0.2195 - accuracy: 0.91 - ETA: 2s - loss: 0.2204 - accuracy: 0.91 - ETA: 2s - loss: 0.2206 - accuracy: 0.91 - ETA: 2s - loss: 0.2195 - accuracy: 0.91 - ETA: 2s - loss: 0.2190 - accuracy: 0.91 - ETA: 2s - loss: 0.2198 - accuracy: 0.91 - ETA: 2s - loss: 0.2203 - accuracy: 0.91 - ETA: 2s - loss: 0.2222 - accuracy: 0.91 - ETA: 2s - loss: 0.2221 - accuracy: 0.91 - ETA: 2s - loss: 0.2227 - accuracy: 0.91 - ETA: 2s - loss: 0.2222 - accuracy: 0.91 - ETA: 1s - loss: 0.2223 - accuracy: 0.91 - ETA: 1s - loss: 0.2224 - accuracy: 0.91 - ETA: 1s - loss: 0.2203 - accuracy: 0.91 - ETA: 1s - loss: 0.2209 - accuracy: 0.91 - ETA: 1s - loss: 0.2213 - accuracy: 0.91 - ETA: 1s - loss: 0.2209 - accuracy: 0.91 - ETA: 1s - loss: 0.2209 - accuracy: 0.91 - ETA: 1s - loss: 0.2210 - accuracy: 0.91 - ETA: 1s - loss: 0.2222 - accuracy: 0.91 - ETA: 1s - loss: 0.2231 - accuracy: 0.91 - ETA: 1s - loss: 0.2226 - accuracy: 0.91 - ETA: 1s - loss: 0.2240 - accuracy: 0.91 - ETA: 1s - loss: 0.2250 - accuracy: 0.91 - ETA: 1s - loss: 0.2251 - accuracy: 0.91 - ETA: 1s - loss: 0.2240 - accuracy: 0.91 - ETA: 1s - loss: 0.2238 - accuracy: 0.91 - ETA: 1s - loss: 0.2238 - accuracy: 0.91 - ETA: 1s - loss: 0.2236 - accuracy: 0.91 - ETA: 1s - loss: 0.2244 - accuracy: 0.91 - ETA: 0s - loss: 0.2243 - accuracy: 0.91 - ETA: 0s - loss: 0.2244 - accuracy: 0.91 - ETA: 0s - loss: 0.2240 - accuracy: 0.91 - ETA: 0s - loss: 0.2236 - accuracy: 0.91 - ETA: 0s - loss: 0.2236 - accuracy: 0.91 - ETA: 0s - loss: 0.2242 - accuracy: 0.91 - ETA: 0s - loss: 0.2238 - accuracy: 0.91 - ETA: 0s - loss: 0.2240 - accuracy: 0.91 - ETA: 0s - loss: 0.2243 - accuracy: 0.91 - ETA: 0s - loss: 0.2243 - accuracy: 0.91 - ETA: 0s - loss: 0.2242 - accuracy: 0.91 - ETA: 0s - loss: 0.2246 - accuracy: 0.91 - ETA: 0s - loss: 0.2249 - accuracy: 0.91 - ETA: 0s - loss: 0.2245 - accuracy: 0.91 - ETA: 0s - loss: 0.2245 - accuracy: 0.91 - ETA: 0s - loss: 0.2244 - accuracy: 0.91 - ETA: 0s - loss: 0.2241 - accuracy: 0.91 - ETA: 0s - loss: 0.2239 - accuracy: 0.91 - ETA: 0s - loss: 0.2242 - accuracy: 0.91 - ETA: 0s - loss: 0.2238 - accuracy: 0.91 - 4s 60us/sample - loss: 0.2237 - accuracy: 0.9144 - val_loss: 0.3364 - val_accuracy: 0.8817\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fdfc73398d0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build the model with the optimal hyperparameters and train it on the data\n",
    "model = tuner.hypermodel.build(best_hps)\n",
    "model.fit(img_train, label_train, epochs = 10, validation_data = (img_test, label_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The my_dir/intro_to_kt directory contains detailed logs and checkpoints for every trial (model configuration) run during the hyperparameter search. If you re-run the hyperparameter search, the Keras Tuner uses the existing state from these logs to resume the search. To disable this behavior, pass an additional overwrite = True argument while instantiating the tuner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Untitled0.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
