# Training and Improving Neural Networks

* Batch gradient descent
* Vanishing / exploding gradients
* Momentum, RMSprop, Adam
* Training / validation / test sets
* Bias-variance tradeoff. Error analysis
* Hyperparameter tuning

## Links
* [Intro to optimization in deep learning: Momentum, RMSProp and Adam](https://medium.com/paperspace/intro-to-optimization-in-deep-learning-momentum-rmsprop-and-adam-8335f15fdee2)
