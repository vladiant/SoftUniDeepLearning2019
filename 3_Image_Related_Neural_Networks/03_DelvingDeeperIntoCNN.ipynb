{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPool2D, BatchNormalization, Flatten, Dense\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "from tensorflow.keras.metrics import SparseCategoricalAccuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image size for the input\n",
    "IMAGE_SIZE = (224, 224)\n",
    "\n",
    "# Image size + channels for the input\n",
    "INPUT_IMAGE_SIZE = (IMAGE_SIZE[0], IMAGE_SIZE[1], 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Delving Deeper into Conolutional Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Input(shape=INPUT_IMAGE_SIZE),\n",
    "    Conv2D(20, 3, padding = \"same\", activation = tf.keras.activations.relu),\n",
    "    Conv2D(20, 3, padding = \"same\", activation = tf.keras.activations.relu),\n",
    "    MaxPool2D(),\n",
    "    Conv2D(50, 3, padding = \"same\", activation = tf.keras.activations.relu),\n",
    "    Conv2D(50, 3, padding = \"same\", activation = tf.keras.activations.relu),\n",
    "    MaxPool2D(),\n",
    "    BatchNormalization(),\n",
    "    Conv2D(100, 3, padding = \"same\", activation = tf.keras.activations.relu),\n",
    "    Conv2D(100, 3, padding = \"same\", activation = tf.keras.activations.relu),\n",
    "    MaxPool2D(),\n",
    "    Flatten(),\n",
    "    Dense(units=20, activation=tf.keras.activations.relu),\n",
    "    Dense(units=2, activation=tf.keras.activations.sigmoid)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 224, 224, 20)      560       \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 224, 224, 20)      3620      \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 112, 112, 20)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 112, 112, 50)      9050      \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 112, 112, 50)      22550     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 56, 56, 50)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 56, 56, 50)        200       \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 56, 56, 100)       45100     \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 56, 56, 100)       90100     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 28, 28, 100)       0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 78400)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 20)                1568020   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 42        \n",
      "=================================================================\n",
      "Total params: 1,739,242\n",
      "Trainable params: 1,739,142\n",
      "Non-trainable params: 100\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer = Adam(learning_rate=0.001),\n",
    "    loss = SparseCategoricalCrossentropy(),\n",
    "    metrics = [SparseCategoricalAccuracy()]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test and train data\n",
    "\n",
    "Download the set from Kaggle\n",
    "\n",
    "https://www.kaggle.com/c/dogs-vs-cats/data\n",
    "\n",
    "Note: Pictures have different sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = os.path.abspath(\"./dogs-vs-cats/train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = []\n",
    "for image_class in os.listdir(BASE_DIR):    \n",
    "    folder_name = os.path.join(BASE_DIR,image_class)\n",
    "    filenames.append(image_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25000"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce the dataset for the example\n",
    "train_filenames = filenames[:90]\n",
    "val_filenames = filenames[90:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_map = {\n",
    "    \"cat\" : 0,\n",
    "    \"dog\" : 1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_classes = [class_map[f[:3]] for f in train_filenames]\n",
    "train_filenames = np.array([os.path.join(BASE_DIR,f) for f in train_filenames])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "90"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_classes = [class_map[f[:3]] for f in val_filenames]\n",
    "val_filenames = np.array([os.path.join(BASE_DIR,f) for f in val_filenames])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(val_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparation of images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(x):\n",
    "    # Normalize\n",
    "    x = tf.cast(x, tf.float32) / 255.0\n",
    "\n",
    "    # 'RGB'->'BGR'\n",
    "    x = x[..., ::-1]\n",
    "    return x\n",
    "\n",
    "def read_and_prepare_image(image_filename, image_class):\n",
    "    # Get images\n",
    "    image = tf.io.read_file(image_filename)\n",
    "    image_decoded = tf.image.decode_jpeg(image)\n",
    "    \n",
    "    # Resize\n",
    "    image_resized = tf.image.resize(image_decoded, IMAGE_SIZE)\n",
    "    \n",
    "    image_tensor = preprocess_image(image_resized)\n",
    "    \n",
    "    return image_tensor, image_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_tf_dataset(filenames, classes, should_batch = True, should_repeat = True):\n",
    "    dataset_initial = tf.data.Dataset.from_tensor_slices((filenames, classes))\n",
    "    dataset_mapped = dataset_initial.map(read_and_prepare_image)\n",
    "    dataset_shuffled = dataset_mapped.shuffle(buffer_size = len(filenames))\n",
    "    \n",
    "    if should_batch:\n",
    "        dataset = dataset_shuffled.batch(BATCH_SIZE)\n",
    "    else:\n",
    "        dataset = dataset_shuffled.batch(len(data))\n",
    "        \n",
    "    if should_repeat:\n",
    "        dataset = dataset.repeat()\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "NUM_EPOCHS = 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = initialize_tf_dataset(train_filenames, train_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_per_epoch_train = round(len(train_filenames) * 1.0 / BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "steps_per_epoch_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data = initialize_tf_dataset(val_filenames, val_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 3 steps\n",
      "Epoch 1/60\n",
      "3/3 [==============================] - 8s 3s/step - loss: 2.3731 - sparse_categorical_accuracy: 0.4889\n",
      "Epoch 2/60\n",
      "3/3 [==============================] - 7s 2s/step - loss: 0.6892 - sparse_categorical_accuracy: 0.5556\n",
      "Epoch 3/60\n",
      "3/3 [==============================] - 7s 2s/step - loss: 0.6856 - sparse_categorical_accuracy: 0.5556\n",
      "Epoch 4/60\n",
      "3/3 [==============================] - 8s 3s/step - loss: 0.6833 - sparse_categorical_accuracy: 0.5556\n",
      "Epoch 5/60\n",
      "3/3 [==============================] - 10s 3s/step - loss: 0.6830 - sparse_categorical_accuracy: 0.5556\n",
      "Epoch 6/60\n",
      "3/3 [==============================] - 9s 3s/step - loss: 0.6691 - sparse_categorical_accuracy: 0.5556\n",
      "Epoch 7/60\n",
      "3/3 [==============================] - 9s 3s/step - loss: 0.6620 - sparse_categorical_accuracy: 0.5556\n",
      "Epoch 8/60\n",
      "3/3 [==============================] - 9s 3s/step - loss: 0.6455 - sparse_categorical_accuracy: 0.5556\n",
      "Epoch 9/60\n",
      "3/3 [==============================] - 9s 3s/step - loss: 0.6087 - sparse_categorical_accuracy: 0.5556\n",
      "Epoch 10/60\n",
      "3/3 [==============================] - 9s 3s/step - loss: 0.5786 - sparse_categorical_accuracy: 0.5556\n",
      "Epoch 11/60\n",
      "3/3 [==============================] - 9s 3s/step - loss: 0.5244 - sparse_categorical_accuracy: 0.5556\n",
      "Epoch 12/60\n",
      "3/3 [==============================] - 9s 3s/step - loss: 0.4971 - sparse_categorical_accuracy: 0.5556\n",
      "Epoch 13/60\n",
      "3/3 [==============================] - 9s 3s/step - loss: 0.5133 - sparse_categorical_accuracy: 0.5556\n",
      "Epoch 14/60\n",
      "3/3 [==============================] - 9s 3s/step - loss: 0.5417 - sparse_categorical_accuracy: 0.5556\n",
      "Epoch 15/60\n",
      "3/3 [==============================] - 9s 3s/step - loss: 0.5800 - sparse_categorical_accuracy: 0.5556\n",
      "Epoch 16/60\n",
      "3/3 [==============================] - 9s 3s/step - loss: 0.4438 - sparse_categorical_accuracy: 0.5556\n",
      "Epoch 17/60\n",
      "3/3 [==============================] - 9s 3s/step - loss: 0.4922 - sparse_categorical_accuracy: 0.5556\n",
      "Epoch 18/60\n",
      "3/3 [==============================] - 9s 3s/step - loss: 0.4505 - sparse_categorical_accuracy: 0.5556\n",
      "Epoch 19/60\n",
      "3/3 [==============================] - 10s 3s/step - loss: 0.4259 - sparse_categorical_accuracy: 0.5556\n",
      "Epoch 20/60\n",
      "3/3 [==============================] - 9s 3s/step - loss: 0.4630 - sparse_categorical_accuracy: 0.5556\n",
      "Epoch 21/60\n",
      "3/3 [==============================] - 9s 3s/step - loss: 0.4024 - sparse_categorical_accuracy: 0.5556\n",
      "Epoch 22/60\n",
      "3/3 [==============================] - 10s 3s/step - loss: 0.3623 - sparse_categorical_accuracy: 0.5556\n",
      "Epoch 23/60\n",
      "3/3 [==============================] - 10s 3s/step - loss: 0.3629 - sparse_categorical_accuracy: 0.5556\n",
      "Epoch 24/60\n",
      "3/3 [==============================] - 9s 3s/step - loss: 0.3476 - sparse_categorical_accuracy: 0.5556\n",
      "Epoch 25/60\n",
      "3/3 [==============================] - 9s 3s/step - loss: 0.3453 - sparse_categorical_accuracy: 0.5556\n",
      "Epoch 26/60\n",
      "3/3 [==============================] - 9s 3s/step - loss: 0.3390 - sparse_categorical_accuracy: 0.5556\n",
      "Epoch 27/60\n",
      "3/3 [==============================] - 9s 3s/step - loss: 0.3314 - sparse_categorical_accuracy: 0.5556\n",
      "Epoch 28/60\n",
      "3/3 [==============================] - 9s 3s/step - loss: 0.3325 - sparse_categorical_accuracy: 0.5556\n",
      "Epoch 29/60\n",
      "3/3 [==============================] - 9s 3s/step - loss: 0.3345 - sparse_categorical_accuracy: 0.5556\n",
      "Epoch 30/60\n",
      "3/3 [==============================] - 9s 3s/step - loss: 0.3222 - sparse_categorical_accuracy: 0.5556\n",
      "Epoch 31/60\n",
      "3/3 [==============================] - 9s 3s/step - loss: 0.3275 - sparse_categorical_accuracy: 0.5556\n",
      "Epoch 32/60\n",
      "3/3 [==============================] - 9s 3s/step - loss: 0.3233 - sparse_categorical_accuracy: 0.5556\n",
      "Epoch 33/60\n",
      "3/3 [==============================] - 9s 3s/step - loss: 0.3161 - sparse_categorical_accuracy: 0.5556\n",
      "Epoch 34/60\n",
      "3/3 [==============================] - 9s 3s/step - loss: 0.3207 - sparse_categorical_accuracy: 0.5556\n",
      "Epoch 35/60\n",
      "3/3 [==============================] - 9s 3s/step - loss: 0.3180 - sparse_categorical_accuracy: 0.5556\n",
      "Epoch 36/60\n",
      "3/3 [==============================] - 9s 3s/step - loss: 0.3179 - sparse_categorical_accuracy: 0.5556\n",
      "Epoch 37/60\n",
      "3/3 [==============================] - 8s 3s/step - loss: 0.3152 - sparse_categorical_accuracy: 0.5556\n",
      "Epoch 38/60\n",
      "3/3 [==============================] - 9s 3s/step - loss: 0.3125 - sparse_categorical_accuracy: 0.5556\n",
      "Epoch 39/60\n",
      "3/3 [==============================] - 9s 3s/step - loss: 0.3150 - sparse_categorical_accuracy: 0.5556\n",
      "Epoch 40/60\n",
      "3/3 [==============================] - 9s 3s/step - loss: 0.3162 - sparse_categorical_accuracy: 0.5556\n",
      "Epoch 41/60\n",
      "3/3 [==============================] - 9s 3s/step - loss: 0.3162 - sparse_categorical_accuracy: 0.5556\n",
      "Epoch 42/60\n",
      "3/3 [==============================] - 9s 3s/step - loss: 0.3161 - sparse_categorical_accuracy: 0.5556\n",
      "Epoch 43/60\n",
      "3/3 [==============================] - 9s 3s/step - loss: 0.3111 - sparse_categorical_accuracy: 0.5556\n",
      "Epoch 44/60\n",
      "3/3 [==============================] - 9s 3s/step - loss: 0.3160 - sparse_categorical_accuracy: 0.5556\n",
      "Epoch 45/60\n",
      "3/3 [==============================] - 9s 3s/step - loss: 0.3173 - sparse_categorical_accuracy: 0.5556\n",
      "Epoch 46/60\n",
      "3/3 [==============================] - 7s 2s/step - loss: 0.3223 - sparse_categorical_accuracy: 0.5556\n",
      "Epoch 47/60\n",
      "3/3 [==============================] - 7s 2s/step - loss: 0.3160 - sparse_categorical_accuracy: 0.5556\n",
      "Epoch 48/60\n",
      "3/3 [==============================] - 7s 2s/step - loss: 0.3148 - sparse_categorical_accuracy: 0.5556\n",
      "Epoch 49/60\n",
      "3/3 [==============================] - 7s 2s/step - loss: 0.3123 - sparse_categorical_accuracy: 0.5556\n",
      "Epoch 50/60\n",
      "3/3 [==============================] - 7s 2s/step - loss: 0.3173 - sparse_categorical_accuracy: 0.5556\n",
      "Epoch 51/60\n",
      "3/3 [==============================] - 7s 2s/step - loss: 0.3172 - sparse_categorical_accuracy: 0.5556\n",
      "Epoch 52/60\n",
      "3/3 [==============================] - 7s 2s/step - loss: 0.3210 - sparse_categorical_accuracy: 0.5556\n",
      "Epoch 53/60\n",
      "3/3 [==============================] - 7s 2s/step - loss: 0.3123 - sparse_categorical_accuracy: 0.5556\n",
      "Epoch 54/60\n",
      "3/3 [==============================] - 7s 2s/step - loss: 0.3123 - sparse_categorical_accuracy: 0.5556\n",
      "Epoch 55/60\n",
      "3/3 [==============================] - 7s 2s/step - loss: 0.3148 - sparse_categorical_accuracy: 0.5556\n",
      "Epoch 56/60\n",
      "3/3 [==============================] - 7s 2s/step - loss: 0.3172 - sparse_categorical_accuracy: 0.5556\n",
      "Epoch 57/60\n",
      "3/3 [==============================] - 7s 2s/step - loss: 0.3185 - sparse_categorical_accuracy: 0.5556\n",
      "Epoch 58/60\n",
      "3/3 [==============================] - 7s 2s/step - loss: 0.3185 - sparse_categorical_accuracy: 0.5556\n",
      "Epoch 59/60\n",
      "3/3 [==============================] - 7s 2s/step - loss: 0.3135 - sparse_categorical_accuracy: 0.5556\n",
      "Epoch 60/60\n",
      "3/3 [==============================] - 7s 2s/step - loss: 0.3160 - sparse_categorical_accuracy: 0.5556\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_data,\n",
    "                    epochs = NUM_EPOCHS,\n",
    "                    steps_per_epoch = steps_per_epoch_train,\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fd19824beb8>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['sparse_categorical_accuracy'], label = 'sparse_categorical_accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc='lower right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 - 0s - loss: 0.7593 - sparse_categorical_accuracy: 0.5000\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(val_data, steps = steps_per_epoch_train, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7592518130938212, 0.5)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_loss, test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
